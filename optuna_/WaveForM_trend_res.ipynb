{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 963\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;66;03m# Assuming you have a device (GPU/CPU) setup\u001b[39;00m\n\u001b[1;32m    962\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 963\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    965\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m  \u001b[38;5;66;03m# or any other number of epochs\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:849\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, buf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 849\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers[key] \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_scatter import scatter_add, scatter_max\n",
    "from pytorch_wavelets import DWT1DForward, DWT1DInverse\n",
    "import warnings\n",
    "\n",
    "from __future__ import division\n",
    "import numbers\n",
    "\n",
    "from torch.nn import Parameter\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional\n",
    "from torch_geometric.utils import to_dense_adj, dense_to_sparse\n",
    "from torch.nn import Sequential, LayerNorm, ReLU\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "from torch.nn import Linear, Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch_geometric.utils import add_self_loops, remove_self_loops, softmax\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn import InstanceNorm1d\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as init\n",
    "\n",
    "import optuna\n",
    "\n",
    "from RevIN import RevIN\n",
    "\n",
    "    \n",
    "class Dataset_Custom(Dataset):\n",
    "    def __init__(self, root_path, flag='train', size=None,\n",
    "                 features='M', data_path='ETTh1.csv',\n",
    "                 target='OT', scale=True, timeenc=0, freq='h',step_size=1):\n",
    "        # Initialize with seq_len and pred_len only\n",
    "        if size is None:\n",
    "            self.seq_len = 96  # Default value, can be adjusted\n",
    "            self.pred_len = 96  # Default value, can be adjusted\n",
    "        else:\n",
    "            self.seq_len, self.pred_len = size[:2]\n",
    "            \n",
    "        \n",
    "        \n",
    "        assert flag in ['train', 'test', 'val']\n",
    "        self.set_type = {'train': 0, 'val': 1, 'test': 2}[flag]\n",
    "\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.scale = scale\n",
    "        self.timeenc = timeenc\n",
    "        self.freq = freq\n",
    "        self.step_size = step_size\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.data_path = data_path\n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        df_raw = pd.read_csv(os.path.join(self.root_path, self.data_path))\n",
    "        \n",
    "        cols_data = [col for col in df_raw.columns if col != 'date']\n",
    "        df_data = df_raw[cols_data]\n",
    "        \n",
    "        num_train = int(len(df_raw) * 0.6)\n",
    "        num_test = int(len(df_raw) * 0.2)\n",
    "        border1s = [0, num_train - self.seq_len, len(df_raw) - num_test - self.seq_len]\n",
    "        border2s = [num_train, num_train + (len(df_raw) - num_train - num_test), len(df_raw)]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "        \n",
    "        if self.features == 'M' or self.features == 'MS':\n",
    "            pass\n",
    "        elif self.features == 'S':\n",
    "            cols_data = df_raw.columns[1:]\n",
    "            df_data = df_raw[cols_data]\n",
    "        \n",
    "        \n",
    "        if self.scale:\n",
    "            train_data = df_data.iloc[border1s[0]:border2s[0]].values\n",
    "            self.scaler.fit(train_data)\n",
    "            data = self.scaler.transform(df_data.values)\n",
    "        else:\n",
    "            data = df_data.values\n",
    "        \n",
    "        df_stamp = df_raw[['date']].iloc[border1:border2]\n",
    "        df_stamp['date'] = pd.to_datetime(df_stamp['date'])\n",
    "        if self.timeenc == 0:\n",
    "            df_stamp['month'] = df_stamp['date'].dt.month\n",
    "            df_stamp['day'] = df_stamp['date'].dt.day\n",
    "            df_stamp['weekday'] = df_stamp['date'].dt.weekday\n",
    "            df_stamp['hour'] = df_stamp['date'].dt.hour\n",
    "            data_stamp = df_stamp.drop(['date'], axis=1).values\n",
    "        elif self.timeenc == 1:\n",
    "            # Example for time_features function\n",
    "            data_stamp = self.time_features(df_stamp['date'], freq=self.freq)\n",
    "\n",
    "        self.data_x = data[border1:border2 - self.pred_len]\n",
    "        self.data_y = data[border1 + self.seq_len:border2]\n",
    "        self.data_stamp = data_stamp\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Calculate the actual start index based on the step size\n",
    "        actual_start_index = index * self.step_size\n",
    "\n",
    "        seq_x = self.data_x[actual_start_index:actual_start_index + self.seq_len]\n",
    "        seq_y = self.data_y[actual_start_index:actual_start_index + self.pred_len]\n",
    "        seq_x_mark = self.data_stamp[actual_start_index:actual_start_index + self.seq_len]\n",
    "        seq_y_mark = self.data_stamp[actual_start_index:actual_start_index + self.pred_len]\n",
    "\n",
    "        return torch.tensor(seq_x,  dtype=torch.float32), torch.tensor(seq_y,  dtype=torch.float32), torch.tensor(seq_x_mark,  dtype=torch.float32), torch.tensor(seq_y_mark,  dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Adjust the total length to account for the step size\n",
    "        total_steps = (len(self.data_x) - self.seq_len - self.pred_len + 1) // self.step_size\n",
    "        return total_steps\n",
    "\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)\n",
    "\n",
    "\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, c_in: int, c_out: int, bias: bool = True):\n",
    "        super(Linear, self).__init__()\n",
    "        self._mlp = torch.nn.Conv2d(\n",
    "            c_in, c_out, kernel_size=(1, 1), padding=(0, 0), stride=(1, 1), bias=bias\n",
    "        )\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "            else:\n",
    "                nn.init.uniform_(p)\n",
    "\n",
    "    def forward(self, X: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        return self._mlp(X)\n",
    "\n",
    "\n",
    "class MixProp(nn.Module):\n",
    "    def __init__(self, c_in: int, c_out: int, gdep: int, dropout: float, alpha: float):\n",
    "        super(MixProp, self).__init__()\n",
    "        self._mlp = Linear((gdep + 1) * c_in, c_out)\n",
    "        self._gdep = gdep\n",
    "        self._dropout = dropout\n",
    "        self._alpha = alpha\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "            else:\n",
    "                nn.init.uniform_(p)\n",
    "\n",
    "    def forward(self, X: torch.FloatTensor, A: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        A = A + torch.eye(A.size(0)).to(X.device)\n",
    "        d = A.sum(1)\n",
    "        H = X\n",
    "        H_0 = X\n",
    "        A = A / d.view(-1, 1)\n",
    "        for _ in range(self._gdep):\n",
    "            \n",
    "            H = self._alpha * X + (1 - self._alpha) * torch.einsum(\n",
    "                \"ncwl,vw->ncvl\", (H, A)\n",
    "            )\n",
    "            H_0 = torch.cat((H_0, H), dim=1)\n",
    "        H_0 = self._mlp(H_0)\n",
    "        return H_0\n",
    "\n",
    "\n",
    "class DilatedInception(nn.Module):\n",
    "    def __init__(self, c_in: int, c_out: int, kernel_set: list, dilation_factor: int):\n",
    "        super(DilatedInception, self).__init__()\n",
    "        self._time_conv = nn.ModuleList()\n",
    "        self._kernel_set = kernel_set\n",
    "        c_out = int(c_out / len(self._kernel_set))\n",
    "        for kern in self._kernel_set:\n",
    "            self._time_conv.append(\n",
    "                nn.Conv2d(c_in, c_out, (1, kern), dilation=(1, dilation_factor))\n",
    "            )\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "            else:\n",
    "                nn.init.uniform_(p)\n",
    "\n",
    "    def forward(self, X_in: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \n",
    "        X = []\n",
    "        for i in range(len(self._kernel_set)):\n",
    "            X.append(self._time_conv[i](X_in))\n",
    "        \n",
    "        for i in range(len(self._kernel_set)):\n",
    "            X[i] = X[i][..., -X[-1].size(3) :]\n",
    "        \n",
    "        \n",
    "        Y = [0 ,0 ,0, 0]\n",
    "        for i in range(len(self._kernel_set)):\n",
    "            Y[i] = X[i].permute(0, 2, 1, 3)\n",
    "        \n",
    "        Y = torch.cat(Y, dim=2)\n",
    "        \n",
    "        X = Y.permute(0, 2, 1, 3)\n",
    "        return  X\n",
    "\n",
    "\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    __constants__ = [\"normalized_shape\", \"weight\", \"bias\", \"eps\", \"elementwise_affine\"]\n",
    "\n",
    "    def __init__(\n",
    "        self, normalized_shape: int, eps: float = 1e-5, elementwise_affine: bool = True\n",
    "    ):\n",
    "        super(LayerNormalization, self).__init__()\n",
    "        self._normalized_shape = tuple(normalized_shape)\n",
    "        self._eps = eps\n",
    "        self._elementwise_affine = elementwise_affine\n",
    "        if self._elementwise_affine:\n",
    "            self._weight = nn.Parameter(torch.Tensor(*normalized_shape))\n",
    "            self._bias = nn.Parameter(torch.Tensor(*normalized_shape))\n",
    "        else:\n",
    "            self.register_parameter(\"_weight\", None)\n",
    "            self.register_parameter(\"_bias\", None)\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        if self._elementwise_affine:\n",
    "            init.ones_(self._weight)\n",
    "            init.zeros_(self._bias)\n",
    "\n",
    "    def forward(self, X: torch.FloatTensor, idx: torch.LongTensor) -> torch.FloatTensor:\n",
    "        if self._elementwise_affine:\n",
    "            return F.layer_norm(\n",
    "                X,\n",
    "                tuple(X.shape[1:]),\n",
    "                self._weight[:, idx, :],\n",
    "                self._bias[:, idx, :],\n",
    "                self._eps,\n",
    "            )\n",
    "        else:\n",
    "            return F.layer_norm(\n",
    "                X, tuple(X.shape[1:]), self._weight, self._bias, self._eps\n",
    "            )\n",
    "\n",
    "\n",
    "class GPModuleLayer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dilation_exponential: int,\n",
    "        rf_size_i: int,\n",
    "        kernel_size: int,\n",
    "        j: int,\n",
    "        residual_channels: int,\n",
    "        conv_channels: int,\n",
    "        skip_channels: int,\n",
    "        kernel_set: list,\n",
    "        new_dilation: int,\n",
    "        layer_norm_affline: bool,\n",
    "        gcn_true: bool,\n",
    "        seq_length: int,\n",
    "        receptive_field: int,\n",
    "        dropout: float,\n",
    "        gcn_depth: int,\n",
    "        num_nodes: int,\n",
    "        propalpha: float,\n",
    "    ):\n",
    "        super(GPModuleLayer, self).__init__()\n",
    "        self._dropout = dropout\n",
    "        self._gcn_true = gcn_true\n",
    "        \n",
    "        if dilation_exponential > 1:\n",
    "            rf_size_j = int(\n",
    "                rf_size_i\n",
    "                + (kernel_size - 1)\n",
    "                * (dilation_exponential ** j - 1)\n",
    "                / (dilation_exponential - 1)\n",
    "            )\n",
    "        else:\n",
    "            rf_size_j = rf_size_i + j * (kernel_size - 1)\n",
    "        \n",
    "        self._filter_conv = DilatedInception(\n",
    "            residual_channels,\n",
    "            conv_channels,\n",
    "            kernel_set=kernel_set,\n",
    "            dilation_factor=new_dilation,\n",
    "        )\n",
    "        \n",
    "        self._gate_conv = DilatedInception(\n",
    "            residual_channels,\n",
    "            conv_channels,\n",
    "            kernel_set=kernel_set,\n",
    "            dilation_factor=new_dilation,\n",
    "        )\n",
    "\n",
    "        self._residual_conv = nn.Conv2d(\n",
    "            in_channels=conv_channels,\n",
    "            out_channels=residual_channels,\n",
    "            kernel_size=(1, 1),\n",
    "        )\n",
    "\n",
    "        if seq_length > receptive_field:\n",
    "            self._skip_conv = nn.Conv2d(\n",
    "                in_channels=conv_channels,\n",
    "                out_channels=skip_channels,\n",
    "                kernel_size=(1, seq_length - rf_size_j + 1),\n",
    "            )\n",
    "        else:\n",
    "            self._skip_conv = nn.Conv2d(\n",
    "                in_channels=conv_channels,\n",
    "                out_channels=skip_channels,\n",
    "                kernel_size=(1, receptive_field - rf_size_j + 1),\n",
    "            )\n",
    "\n",
    "        if gcn_true:\n",
    "            self._mixprop_conv1 = MixProp(\n",
    "                conv_channels, residual_channels, gcn_depth, dropout, propalpha\n",
    "            )\n",
    "\n",
    "            self._mixprop_conv2 = MixProp(\n",
    "                conv_channels, residual_channels, gcn_depth, dropout, propalpha\n",
    "            )\n",
    "\n",
    "        if seq_length > receptive_field:\n",
    "            self._normalization = LayerNormalization(\n",
    "                (residual_channels, num_nodes, seq_length - rf_size_j + 1),\n",
    "                elementwise_affine=layer_norm_affline,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self._normalization = LayerNormalization(\n",
    "                (residual_channels, num_nodes, receptive_field - rf_size_j + 1),\n",
    "                elementwise_affine=layer_norm_affline,\n",
    "            )\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "            else:\n",
    "                nn.init.uniform_(p)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        X: torch.FloatTensor,\n",
    "        X_skip: torch.FloatTensor,\n",
    "        A_tilde: Optional[torch.FloatTensor],\n",
    "        idx: torch.LongTensor,\n",
    "        training: bool,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \n",
    "        X_residual = X\n",
    "        X_filter = self._filter_conv(X)\n",
    "        X_filter = torch.tanh(X_filter)\n",
    "        X_gate = self._gate_conv(X)\n",
    "        X_gate = torch.sigmoid(X_gate)\n",
    "        X = X_filter * X_gate\n",
    "        X = F.dropout(X, self._dropout, training=training)\n",
    "        X_skip = self._skip_conv(X) + X_skip\n",
    "        if self._gcn_true:\n",
    "            X = self._mixprop_conv1(X, A_tilde) + self._mixprop_conv2(\n",
    "                X, A_tilde.transpose(1, 0)\n",
    "            )\n",
    "        else:\n",
    "            X = self._residual_conv(X)\n",
    "\n",
    "        X = X + X_residual[:, :, :, -X.size(3) :]\n",
    "        X = self._normalization(X, idx)\n",
    "        return X, X_skip\n",
    "\n",
    "\n",
    "class GPModule(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        gcn_true: bool,\n",
    "        build_adj: bool,\n",
    "        gcn_depth: int,\n",
    "        num_nodes: int,\n",
    "        kernel_set: list,\n",
    "        kernel_size: int,\n",
    "        dropout: float,\n",
    "        dilation_exponential: int,\n",
    "        conv_channels: int,\n",
    "        residual_channels: int,\n",
    "        skip_channels: int,\n",
    "        end_channels: int,\n",
    "        seq_length: int,\n",
    "        in_dim: int,\n",
    "        out_dim: int,\n",
    "        layers: int,\n",
    "        propalpha: float,\n",
    "        layer_norm_affline: bool,\n",
    "        graph_constructor,\n",
    "        xd: Optional[int] = None,\n",
    "    ):\n",
    "        super(GPModule, self).__init__()\n",
    "        \n",
    "        self._gcn_true = gcn_true\n",
    "        self._build_adj_true = build_adj\n",
    "        self._num_nodes = num_nodes\n",
    "        self._dropout = dropout\n",
    "        self._seq_length = seq_length\n",
    "        self._layers = layers\n",
    "        self._idx = torch.arange(self._num_nodes)\n",
    "        \n",
    "        self._gp_layers = nn.ModuleList()\n",
    "        \n",
    "        self._graph_constructor = graph_constructor\n",
    "        \n",
    "        self._set_receptive_field(dilation_exponential, kernel_size, layers)\n",
    "        \n",
    "        new_dilation = 1\n",
    "        for j in range(1, layers + 1):\n",
    "            self._gp_layers.append(\n",
    "                GPModuleLayer(\n",
    "                    dilation_exponential=dilation_exponential,\n",
    "                    rf_size_i=1,\n",
    "                    kernel_size=kernel_size,\n",
    "                    j=j,\n",
    "                    residual_channels=residual_channels,\n",
    "                    conv_channels=conv_channels,\n",
    "                    skip_channels=skip_channels,\n",
    "                    kernel_set=kernel_set,\n",
    "                    new_dilation=new_dilation,\n",
    "                    layer_norm_affline=layer_norm_affline,\n",
    "                    gcn_true=gcn_true,\n",
    "                    seq_length=seq_length,\n",
    "                    receptive_field=self._receptive_field,\n",
    "                    dropout=dropout,\n",
    "                    gcn_depth=gcn_depth,\n",
    "                    num_nodes=num_nodes,\n",
    "                    propalpha=propalpha,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            new_dilation *= dilation_exponential\n",
    "        \n",
    "        self._setup_conv(\n",
    "            in_dim, skip_channels, end_channels, residual_channels, out_dim\n",
    "        )\n",
    "        \n",
    "        self._reset_parameters()\n",
    "        \n",
    "    def _setup_conv(\n",
    "        self, in_dim, skip_channels, end_channels, residual_channels, out_dim\n",
    "    ):\n",
    "    \n",
    "        self._start_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=residual_channels, kernel_size=(1, 1)\n",
    "        )\n",
    "        \n",
    "        if self._seq_length > self._receptive_field:\n",
    "            \n",
    "            self._skip_conv_0 = nn.Conv2d(\n",
    "                in_channels=in_dim,\n",
    "                out_channels=skip_channels,\n",
    "                kernel_size=(1, self._seq_length),\n",
    "                bias=True,\n",
    "            )\n",
    "            \n",
    "            self._skip_conv_E = nn.Conv2d(\n",
    "                in_channels=residual_channels,\n",
    "                out_channels=skip_channels,\n",
    "                kernel_size=(1, self._seq_length - self._receptive_field + 1),\n",
    "                bias=True,\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            self._skip_conv_0 = nn.Conv2d(\n",
    "                in_channels=in_dim,\n",
    "                out_channels=skip_channels,\n",
    "                kernel_size=(1, self._receptive_field),\n",
    "                bias=True,\n",
    "            )\n",
    "            \n",
    "            self._skip_conv_E = nn.Conv2d(\n",
    "                in_channels=residual_channels,\n",
    "                out_channels=skip_channels,\n",
    "                kernel_size=(1, 1),\n",
    "                bias=True,\n",
    "            )\n",
    "        \n",
    "        self._end_conv_1 = nn.Conv2d(\n",
    "            in_channels=skip_channels,\n",
    "            out_channels=end_channels,\n",
    "            kernel_size=(1, 1),\n",
    "            bias=True,\n",
    "        )\n",
    "        \n",
    "        self._end_conv_2 = nn.Conv2d(\n",
    "            in_channels=end_channels,\n",
    "            out_channels=out_dim,\n",
    "            kernel_size=(1, 1),\n",
    "            bias=True,\n",
    "        )\n",
    "    \n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "            else:\n",
    "                nn.init.uniform_(p)\n",
    "    \n",
    "    def _set_receptive_field(self, dilation_exponential, kernel_size, layers):\n",
    "        if dilation_exponential > 1:\n",
    "            self._receptive_field = int(\n",
    "                1\n",
    "                + (kernel_size - 1)\n",
    "                * (dilation_exponential ** layers - 1)\n",
    "                / (dilation_exponential - 1)\n",
    "            )\n",
    "        else:\n",
    "            self._receptive_field = layers * (kernel_size - 1) + 1\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        context: torch.FloatTensor,\n",
    "        A_tilde: Optional[torch.FloatTensor] = None,\n",
    "        idx: Optional[torch.LongTensor] = None,\n",
    "        FE: Optional[torch.FloatTensor] = None,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \n",
    "        X_in = context.permute(0, 3, 2, 1)\n",
    "        \n",
    "        seq_len = X_in.size(3)\n",
    "        assert (\n",
    "            seq_len == self._seq_length\n",
    "        ), \"Input sequence length not equal to preset sequence length.\"\n",
    "        \n",
    "        if self._seq_length < self._receptive_field:\n",
    "            X_in = nn.functional.pad(\n",
    "                X_in, (self._receptive_field - self._seq_length, 0, 0, 0)\n",
    "            )\n",
    "        \n",
    "        if self._gcn_true:\n",
    "            if self._build_adj_true:\n",
    "                if idx is None:\n",
    "                    A_tilde = self._graph_constructor(self._idx.to(X_in.device), FE=FE)\n",
    "                else:\n",
    "                    A_tilde = self._graph_constructor(idx, FE=FE)\n",
    "        \n",
    "        X = self._start_conv(X_in)\n",
    "        X_skip = self._skip_conv_0(\n",
    "            F.dropout(X_in, self._dropout, training=self.training)\n",
    "        )\n",
    "        if idx is None:\n",
    "            for gp in self._gp_layers:\n",
    "                \n",
    "                X, X_skip = gp(X, X_skip, A_tilde, self._idx.to(X_in.device), self.training)\n",
    "        else:\n",
    "            for gp in self._gp_layers:\n",
    "                X, X_skip = gp(X, X_skip, A_tilde, idx, self.training)\n",
    "        \n",
    "        X_skip = self._skip_conv_E(X) + X_skip\n",
    "        X = F.relu(X_skip)\n",
    "        X = F.relu(self._end_conv_1(X))\n",
    "        X = self._end_conv_2(X)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    \n",
    "    \n",
    "    def load_my_state_dict(self, state_dict):\n",
    "        own_state = self.state_dict()\n",
    "        for name, param in state_dict.items():\n",
    "            if isinstance(param, Parameter):\n",
    "                param = param.data\n",
    "            try:\n",
    "                own_state[name].copy_(param)\n",
    "            except:\n",
    "                print(name)\n",
    "                print(param.shape)\n",
    "\n",
    "\n",
    "class GraphConstructor(nn.Module):\n",
    "    def __init__(\n",
    "        self, nnodes: int, k: int, dim: int, alpha: float, xd: Optional[int] = None\n",
    "    ):\n",
    "        super(GraphConstructor, self).__init__()\n",
    "        if xd is not None:\n",
    "            self._static_feature_dim = xd\n",
    "            self._linear1 = nn.Linear(xd, dim)\n",
    "            self._linear2 = nn.Linear(xd, dim)\n",
    "        else:\n",
    "            self._embedding1 = nn.Embedding(nnodes, dim)\n",
    "            self._embedding2 = nn.Embedding(nnodes, dim)\n",
    "            self._linear1 = nn.Linear(dim, dim)\n",
    "            self._linear2 = nn.Linear(dim, dim)\n",
    "        \n",
    "        self._k = k\n",
    "        self._alpha = alpha\n",
    "        \n",
    "        self._reset_parameters()\n",
    "        \n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "            else:\n",
    "                nn.init.uniform_(p)\n",
    "\n",
    "    def forward(\n",
    "        self, idx: torch.LongTensor, FE: Optional[torch.FloatTensor] = None\n",
    "    ) -> torch.FloatTensor:\n",
    "        \n",
    "        if FE is None:\n",
    "            nodevec1 = self._embedding1(idx)\n",
    "            nodevec2 = self._embedding2(idx)\n",
    "        else:\n",
    "            assert FE.shape[1] == self._static_feature_dim\n",
    "            nodevec1 = FE[idx, :]\n",
    "            nodevec2 = nodevec1 \n",
    "        \n",
    "        nodevec1 = torch.tanh(self._alpha * self._linear1(nodevec1))\n",
    "        nodevec2 = torch.tanh(self._alpha * self._linear2(nodevec2))\n",
    "    \n",
    "        a = torch.mm(nodevec1, nodevec2.transpose(1, 0)) - torch.mm(\n",
    "            nodevec2, nodevec1.transpose(1, 0)\n",
    "        )\n",
    "        A = F.relu(torch.tanh(self._alpha * a))\n",
    "        mask = torch.zeros(idx.size(0), idx.size(0)).to(A.device)\n",
    "        mask.fill_(float(\"0\"))\n",
    "        \n",
    "        s1, t1 = A.topk(self._k, 1)\n",
    "        mask.scatter_(1, t1, s1.fill_(1))\n",
    "        A = A * mask\n",
    "        return A\n",
    "\n",
    "class Model_(nn.Module):\n",
    "    def __init__(self, seq_len, pred_len, n_points, dropout, wavelet_j, wavelet, subgraph_size, node_dim, n_gnn_layer):\n",
    "        super(Model_, self).__init__()\n",
    "        self.seq_len = seq_len # Sequence Length\n",
    "        self.pred_len = pred_len # Pred Length\n",
    "        self.points = n_points # Num Features\n",
    "        self.dropout = dropout #dropout rate\n",
    "        \n",
    "        decompose_layer = wavelet_j # the number of wavelet decompose layer\n",
    "        wave = wavelet # wavelet function e.g.) 'haar'\n",
    "        \n",
    "        mode = 'symmetric' \n",
    "        self.dwt = DWT1DForward(wave=wave, J=decompose_layer, mode=mode)  \n",
    "        self.idwt = DWT1DInverse(wave=wave)\n",
    "        \n",
    "        \n",
    "        tmp1 = torch.randn(1, 1, self.seq_len)\n",
    "        tmp1_yl, tmp1_yh = self.dwt(tmp1)\n",
    "        tmp1_coefs = [tmp1_yl] + tmp1_yh\n",
    "        \n",
    "        tmp2 = torch.randn(1, 1, self.seq_len + self.pred_len)\n",
    "        tmp2_yl, tmp2_yh = self.dwt(tmp2)\n",
    "        tmp2_coefs = [tmp2_yl] + tmp2_yh\n",
    "        assert decompose_layer + 1 == len(tmp1_coefs) == len(tmp2_coefs)\n",
    "        \n",
    "        self._graph_constructor = GraphConstructor(\n",
    "            nnodes=self.points,\n",
    "            k=subgraph_size, # topk\n",
    "            dim=node_dim, # node_dim in graph\n",
    "            alpha=3.0\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.nets = nn.ModuleList()\n",
    "        for i in range(decompose_layer + 1):\n",
    "            self.nets.append(\n",
    "                GPModule(\n",
    "                    gcn_true = True,\n",
    "                    build_adj = True,\n",
    "                    gcn_depth=2,\n",
    "                    num_nodes=self.points,\n",
    "                    kernel_set=[2, 3, 6, 7],\n",
    "                    kernel_size=7,\n",
    "                    dropout=self.dropout,\n",
    "                    conv_channels=32,\n",
    "                    residual_channels=32,\n",
    "                    skip_channels=64,\n",
    "                    end_channels=128,\n",
    "                    seq_length=(tmp1_coefs[i].shape[-1]),\n",
    "                    in_dim=1,\n",
    "                    out_dim=(tmp2_coefs[i].shape[-1]) - (tmp1_coefs[i].shape[-1]),\n",
    "                    layers=n_gnn_layer, # the nubmer of layers of gnn\n",
    "                    propalpha=0.05,\n",
    "                    dilation_exponential=2,\n",
    "                    graph_constructor=self._graph_constructor,\n",
    "                    layer_norm_affline=True,\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    \n",
    "    def model(self, coefs):\n",
    "        new_coefs = []\n",
    "        for coef, net in zip(coefs, self.nets):\n",
    "            new_coef = net(coef.permute(0,2,1).unsqueeze(-1))\n",
    "            new_coefs.append(new_coef.squeeze().permute(0,2,1))\n",
    "        \n",
    "        return new_coefs\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, x_enc):\n",
    "        in_dwt = x_enc.permute(0,2,1)\n",
    "        \n",
    "        yl, yhs = self.dwt(in_dwt)\n",
    "        coefs = [yl] + yhs\n",
    "        \n",
    "        \n",
    "        coefs_new = self.model(coefs)\n",
    "        \n",
    "        coefs_idwt = []\n",
    "        for i in range(len(coefs_new)):\n",
    "            coefs_idwt.append(torch.cat((coefs[i], coefs_new[i]), 2))\n",
    "        \n",
    "        \n",
    "        out = self.idwt((coefs_idwt[0], coefs_idwt[1:]))\n",
    "        pred_out = out.permute(0, 2, 1)\n",
    "        \n",
    "        \n",
    "        return pred_out[:, -self.pred_len:, :]\n",
    "\n",
    "class moving_avg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "    \n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, seq_len, pred_len, n_points, dropout, wavelet_j, wavelet, subgraph_size, node_dim, n_gnn_layer):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # Initialize the backbone Model\n",
    "        self.backbone_res = Model_(\n",
    "            seq_len=seq_len,\n",
    "            pred_len=pred_len,\n",
    "            n_points=n_points,\n",
    "            dropout=dropout,\n",
    "            wavelet_j=wavelet_j,\n",
    "            wavelet=wavelet,\n",
    "            subgraph_size=subgraph_size,\n",
    "            node_dim=node_dim,\n",
    "            n_gnn_layer=n_gnn_layer\n",
    "        )\n",
    "        \n",
    "        self.backbone_trend = Model_(\n",
    "            seq_len=seq_len,\n",
    "            pred_len=pred_len,\n",
    "            n_points=n_points,\n",
    "            dropout=dropout,\n",
    "            wavelet_j=wavelet_j,\n",
    "            wavelet=wavelet,\n",
    "            subgraph_size=subgraph_size,\n",
    "            node_dim=node_dim,\n",
    "            n_gnn_layer=n_gnn_layer\n",
    "        )\n",
    "        \n",
    "   \n",
    "        self.series_decomp = series_decomp(kernel_size=25)\n",
    "    def forward(self, x):\n",
    "        res, trend = self.series_decomp(x)\n",
    "        # Pass input through the backbone model\n",
    "        pred_out_res = self.backbone_res(res)\n",
    "        pred_out_trend = self.backbone_trend(trend)\n",
    "        output = pred_out_res + pred_out_trend\n",
    "        \n",
    "        return output\n",
    "\n",
    "    \n",
    "    \n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for seq_x, seq_y, seq_x_mark, seq_y_mark in test_loader:\n",
    "            seq_x, seq_y = seq_x.to(device), seq_y.to(device)\n",
    "            outputs = model(seq_x)\n",
    "            loss = criterion(outputs, seq_y)\n",
    "            total_loss += loss.item() * seq_x.size(0)\n",
    "    return total_loss / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for seq_x, seq_y, seq_x_mark, seq_y_mark in train_loader:\n",
    "        seq_x, seq_y = seq_x.to(device), seq_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(seq_x)\n",
    "        loss = criterion(outputs, seq_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * seq_x.size(0)\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for seq_x, seq_y, seq_x_mark, seq_y_mark in val_loader:\n",
    "            seq_x, seq_y = seq_x.to(device), seq_y.to(device)\n",
    "            outputs = model(seq_x)\n",
    "            loss = criterion(outputs, seq_y)\n",
    "            total_loss += loss.item() * seq_x.size(0)\n",
    "    return total_loss / len(val_loader.dataset)\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                           Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "    \n",
    "# seq_len, pred_len, n_points, dropout, wavelet_j, wavelet, subgraph_size, node_dim, n_gnn_layer\n",
    "# Assuming DWT_MLP_Model is defined elsewhere, along with the necessary imports\n",
    "seq_ = 24*4\n",
    "pred_ = 24*4\n",
    "n_points = 321\n",
    "wavelet_j = 2\n",
    "wavelet = 'haar'\n",
    "dropout_rate = 0.05\n",
    "supbraph_size = 6\n",
    "node_dim = 40\n",
    "n_gnn_layer = 3\n",
    "s_size = 5\n",
    "indices = [0,1,2,3,4,5] #np.random.choice(range(100), size=3, replace=False)\n",
    "input_length = [24*4, 512]\n",
    "\n",
    "\n",
    "\n",
    "for i in indices:\n",
    "    if i < 3:\n",
    "        seq_length = input_length[0]\n",
    "    else:\n",
    "        seq_length = input_length[1]\n",
    "        \n",
    "    # Specify the file path\n",
    "    root_path = '/home/choi/Wave_Transformer/optuna_/electricity/'\n",
    "    data_path = 'electricity.csv'\n",
    "    # Size parameters\n",
    "\n",
    "    seq_len = seq_length # 24*4*4\n",
    "    pred_len = 24*4\n",
    "    #batch_size = bs\n",
    "    # Initialize the custom dataset for training, validation, and testing\n",
    "    train_dataset = Dataset_Custom(root_path=root_path, features= 'M', flag='train', data_path=data_path, step_size =s_size)\n",
    "    val_dataset = Dataset_Custom(root_path=root_path, features= 'M',flag='val', data_path=data_path,step_size = s_size)\n",
    "    test_dataset = Dataset_Custom(root_path=root_path, features= 'M',flag='test', data_path=data_path,step_size = s_size)\n",
    "\n",
    "    # Optionally, initialize the dataset for prediction (if needed)\n",
    "    #pred_dataset = Dataset_Pred(root_path=root_path, flag='pred', size=size, data_path=data_path, inverse=True)\n",
    "\n",
    "    # Example on how to create DataLoaders for PyTorch training (adjust batch_size as needed)\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last = True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last = False)\n",
    "    dropout_rate = dropout_rate\n",
    "    \n",
    "    model = Model(\n",
    "    seq_len=seq_len, \n",
    "    pred_len=pred_len, \n",
    "    n_points=n_points, \n",
    "    dropout=dropout_rate, \n",
    "    wavelet_j=wavelet_j, \n",
    "    wavelet=wavelet, \n",
    "    subgraph_size=supbraph_size, \n",
    "    node_dim=node_dim, \n",
    "    n_gnn_layer=n_gnn_layer\n",
    ")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Assuming you have a device (GPU/CPU) setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "    num_epochs = 50  # or any other number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = validate(model, val_loader, criterion, device)\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Call early stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "    test_loss = test(model, test_loader, criterion, device)\n",
    "    print(f'Test Loss: {test_loss:.4f}')\n",
    "\n",
    "\n",
    "        \n",
    "      \n",
    "    \n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
