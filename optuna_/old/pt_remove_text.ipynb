{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the original log file in read mode and a new file in write mode\n",
    "with open('/home/choi/Wave_Transformer/optuna_/py_torch_test630796.log', 'r') as read_file, open('filtered_py_torch_test630796.log', 'w') as write_file:\n",
    "    # Iterate over each line in the original file\n",
    "    for line in read_file:\n",
    "        # Check if '.pt' is not in the current line\n",
    "        if '.pt' not in line:\n",
    "            # Write the line to the new file if '.pt' is not found\n",
    "            write_file.write(line)\n",
    "\n",
    "# After running this script, 'filtered_log_file.log' will contain all the lines\n",
    "# from the original log file except those that contain '.pt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\n",
    "    'filtered_py_torch_test630796.log',\n",
    "    'filtered_py_torch_test630782.log',\n",
    "    'filtered_py_torch_test630781.log',\n",
    "    'filtered_py_torch_test630779.log'\n",
    "]\n",
    "\n",
    "with open('merged_test_losses.log', 'w') as output_file:\n",
    "    for file_name in file_names:\n",
    "        with open(file_name, 'r') as read_file:\n",
    "            for line in read_file:\n",
    "                if line.startswith('Test Loss'):\n",
    "                    output_file.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted lines have been written to sorted_merged_test_losses.log.\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the log file\n",
    "log_file_path = 'merged_test_losses.log'\n",
    "sorted_log_file_path = 'sorted_merged_test_losses.log'\n",
    "\n",
    "# Read the lines from the log file\n",
    "with open(log_file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Extract test loss values and associate them with their lines\n",
    "lines_with_loss = []\n",
    "for line in lines:\n",
    "    # Find the test loss value in each line and convert it to float\n",
    "    try:\n",
    "        loss_value = float(line.strip().split(': ')[-1])\n",
    "        lines_with_loss.append((loss_value, line))\n",
    "    except ValueError:\n",
    "        # Handle the case where conversion to float fails\n",
    "        print(\"Error converting loss value to float in line:\", line)\n",
    "\n",
    "# Sort the lines by the test loss values in ascending order\n",
    "lines_with_loss.sort(key=lambda x: x[0])\n",
    "\n",
    "# Write the sorted lines back to a new file\n",
    "with open(sorted_log_file_path, 'w') as file:\n",
    "    for _, line in lines_with_loss:\n",
    "        file.write(line)\n",
    "\n",
    "print(f'Sorted lines have been written to {sorted_log_file_path}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to the sorted log file\n",
    "sorted_log_file_path = 'sorted_merged_test_losses.log'\n",
    "\n",
    "# Read the log file into a DataFrame\n",
    "# Assuming each line in the log file is a separate entry in the DataFrame\n",
    "df = pd.read_csv(sorted_log_file_path, header=None, names=['Log Entry'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the log file into a DataFrame assuming the path 'sorted_merged_test_losses.log'\n",
    "df = pd.read_csv('sorted_merged_test_losses.log', sep=',', header=None)\n",
    "\n",
    "# Drop the column that contains the repeated phrase 'Test Loss for configuration:'\n",
    "df = df.drop(0, axis=1)\n",
    "\n",
    "# Function to extract column names and their values, ensuring Test Loss is correctly captured\n",
    "def extract_columns_and_values(row):\n",
    "    config_dict = {}\n",
    "    for item in row:\n",
    "        if pd.isnull(item):\n",
    "            continue\n",
    "        if ':' in item:  # This indicates the presence of a Test Loss value\n",
    "            key, value = item.rsplit(':', 1)  # Split from the right to correctly capture Test Loss\n",
    "            key = key.split('=')[-1].strip()  # Ensure the key is correctly extracted in case of Test Loss\n",
    "        else:\n",
    "            key, value = item.split('=')\n",
    "            key = key.strip()\n",
    "            value = value.strip()\n",
    "        config_dict[key] = value\n",
    "    return pd.Series(config_dict)\n",
    "\n",
    "# Apply the function to each row\n",
    "df_transformed = df.apply(extract_columns_and_values, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['1', '3', '5', 'Data Load Type', 'Pos_Encoder_Type', 'Revin',\n",
      "       'attention_type', 'batch_size', 'dropout_enabled', 'general_skip',\n",
      "       'kernel_size', 'skip_enabled', 'step_size'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check if your DataFrame column names are correctly referenced\n",
    "print(df_transformed.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the '1', '3', '5' columns into a single 'Test Loss' column\n",
    "# First, fill NaN values in column '1' with values from '3', then with values from '5'\n",
    "df_transformed['Test Loss'] = df_transformed['1'].fillna(df_transformed['3']).fillna(df_transformed['5'])\n",
    "\n",
    "# Drop the original columns '1', '3', '5'\n",
    "df_transformed = df_transformed.drop(columns=['1', '3', '5'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Load Type</th>\n",
       "      <th>Pos_Encoder_Type</th>\n",
       "      <th>Revin</th>\n",
       "      <th>attention_type</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout_enabled</th>\n",
       "      <th>general_skip</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>skip_enabled</th>\n",
       "      <th>step_size</th>\n",
       "      <th>Test Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>multivariate</td>\n",
       "      <td>Original</td>\n",
       "      <td>True</td>\n",
       "      <td>log</td>\n",
       "      <td>128</td>\n",
       "      <td>True</td>\n",
       "      <td>skip</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>multivariate</td>\n",
       "      <td>Projected</td>\n",
       "      <td>True</td>\n",
       "      <td>original</td>\n",
       "      <td>64</td>\n",
       "      <td>True</td>\n",
       "      <td>skip</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>multivariate</td>\n",
       "      <td>Projected</td>\n",
       "      <td>True</td>\n",
       "      <td>original</td>\n",
       "      <td>128</td>\n",
       "      <td>True</td>\n",
       "      <td>skip</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>multivariate</td>\n",
       "      <td>Projected</td>\n",
       "      <td>True</td>\n",
       "      <td>original</td>\n",
       "      <td>128</td>\n",
       "      <td>True</td>\n",
       "      <td>skip</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>multivariate</td>\n",
       "      <td>Projected</td>\n",
       "      <td>True</td>\n",
       "      <td>original</td>\n",
       "      <td>64</td>\n",
       "      <td>True</td>\n",
       "      <td>skip</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>multivariate</td>\n",
       "      <td>Original</td>\n",
       "      <td>True</td>\n",
       "      <td>log</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>skip</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>1.0695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>multivariate</td>\n",
       "      <td>Original</td>\n",
       "      <td>True</td>\n",
       "      <td>log</td>\n",
       "      <td>128</td>\n",
       "      <td>True</td>\n",
       "      <td>skip</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>1.0724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>multivariate</td>\n",
       "      <td>Original</td>\n",
       "      <td>True</td>\n",
       "      <td>log</td>\n",
       "      <td>128</td>\n",
       "      <td>True</td>\n",
       "      <td>skip</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>1.0734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>multivariate</td>\n",
       "      <td>Original</td>\n",
       "      <td>True</td>\n",
       "      <td>log</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>skip</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>1.0803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>multivariate</td>\n",
       "      <td>Original</td>\n",
       "      <td>True</td>\n",
       "      <td>log</td>\n",
       "      <td>64</td>\n",
       "      <td>True</td>\n",
       "      <td>skip</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>1.0803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>407 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Data Load Type Pos_Encoder_Type Revin attention_type batch_size  \\\n",
       "0     multivariate         Original  True            log        128   \n",
       "1     multivariate        Projected  True       original         64   \n",
       "2     multivariate        Projected  True       original        128   \n",
       "3     multivariate        Projected  True       original        128   \n",
       "4     multivariate        Projected  True       original         64   \n",
       "..             ...              ...   ...            ...        ...   \n",
       "402   multivariate         Original  True            log         32   \n",
       "403   multivariate         Original  True            log        128   \n",
       "404   multivariate         Original  True            log        128   \n",
       "405   multivariate         Original  True            log         32   \n",
       "406   multivariate         Original  True            log         64   \n",
       "\n",
       "    dropout_enabled general_skip kernel_size skip_enabled step_size Test Loss  \n",
       "0              True         skip           5         True         1    0.2045  \n",
       "1              True         skip           5         True         1    0.2051  \n",
       "2              True         skip           5         True         1    0.2055  \n",
       "3              True         skip           3         True         1    0.2056  \n",
       "4              True         skip           5         True         1    0.2060  \n",
       "..              ...          ...         ...          ...       ...       ...  \n",
       "402            True         skip           7         True        64    1.0695  \n",
       "403            True         skip           5         True        64    1.0724  \n",
       "404            True         skip           5         True        64    1.0734  \n",
       "405            True         skip           5         True        64    1.0803  \n",
       "406            True         skip           5         True        64    1.0803  \n",
       "\n",
       "[407 rows x 11 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping for 'attention_type': {0: 'log', 1: 'original'}\n"
     ]
    }
   ],
   "source": [
    "## Example for 'attention_type'\n",
    "attention_type_mapping = df_transformed['attention_type'].astype('category').cat.categories\n",
    "print(\"Mapping for 'attention_type':\", dict(enumerate(attention_type_mapping)))\n",
    "\n",
    "# \n",
    "# Convert 'Test Loss' to numeric, ensuring all values are treated as floats\n",
    "df_transformed['Test Loss'] = pd.to_numeric(df_transformed['Test Loss'], errors='coerce')\n",
    "\n",
    "# Group by 'attention_type' and calculate the mean of 'Test Loss' for each category\n",
    "attention_type_avg_loss = df_transformed.groupby('attention_type')['Test Loss'].mean()\n",
    "\n",
    "# Sort the results to see which 'attention_type' is associated with lower 'Test Loss'\n",
    "sorted_attention_type_avg_loss = attention_type_avg_loss.sort_values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attention_type\n",
       "original    0.281924\n",
       "log         0.412857\n",
       "Name: Test Loss, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_attention_type_avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean 'Test Loss' for each category in Data Load Type:\n",
      "Data Load Type\n",
      "multivariate    0.36726\n",
      "univariate      0.98770\n",
      "Name: Test Loss, dtype: float64 \n",
      "\n",
      "Mean 'Test Loss' for each category in Pos_Encoder_Type:\n",
      "Pos_Encoder_Type\n",
      "Projected    0.355617\n",
      "Original     0.389321\n",
      "Name: Test Loss, dtype: float64 \n",
      "\n",
      "Mean 'Test Loss' for each category in attention_type:\n",
      "attention_type\n",
      "original    0.281924\n",
      "log         0.412857\n",
      "Name: Test Loss, dtype: float64 \n",
      "\n",
      "Mean 'Test Loss' for each category in general_skip:\n",
      "general_skip\n",
      "skip    0.368784\n",
      "Name: Test Loss, dtype: float64 \n",
      "\n",
      "Mean 'Test Loss' for each category in dropout_enabled:\n",
      "dropout_enabled\n",
      "True    0.368784\n",
      "Name: Test Loss, dtype: float64 \n",
      "\n",
      "Mean 'Test Loss' for each category in skip_enabled:\n",
      "skip_enabled\n",
      "True    0.368784\n",
      "Name: Test Loss, dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean 'Test Loss' for each category in all categorical columns\n",
    "categorical_columns = ['Data Load Type', 'Pos_Encoder_Type', 'attention_type', 'general_skip', 'dropout_enabled', 'skip_enabled']\n",
    "for col in categorical_columns:\n",
    "    print(f\"Mean 'Test Loss' for each category in {col}:\")\n",
    "    print(df_transformed.groupby(col)['Test Loss'].mean().sort_values(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlations between numeric features and 'Test Loss':\n",
      "kernel_size    0.075527\n",
      "batch_size     0.097493\n",
      "step_size      0.835938\n",
      "Name: Test Loss, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Ensure 'Test Loss' and other numeric columns are in the correct data type\n",
    "numeric_columns = ['batch_size', 'kernel_size', 'step_size']  # Add other numeric columns as needed\n",
    "df_transformed[numeric_columns] = df_transformed[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Calculate correlations\n",
    "numeric_correlations = df_transformed[numeric_columns + ['Test Loss']].corr()['Test Loss']\n",
    "print(\"Correlations between numeric features and 'Test Loss':\")\n",
    "print(numeric_correlations.drop('Test Loss', axis=0).sort_values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Data Load Type Pos_Encoder_Type Revin attention_type  batch_size  \\\n",
      "0   multivariate         Original  True            log         128   \n",
      "1   multivariate        Projected  True       original          64   \n",
      "2   multivariate        Projected  True       original         128   \n",
      "3   multivariate        Projected  True       original         128   \n",
      "4   multivariate        Projected  True       original          64   \n",
      "\n",
      "  dropout_enabled general_skip  kernel_size skip_enabled  step_size  Test Loss  \n",
      "0            True         skip            5         True          1     0.2045  \n",
      "1            True         skip            5         True          1     0.2051  \n",
      "2            True         skip            5         True          1     0.2055  \n",
      "3            True         skip            3         True          1     0.2056  \n",
      "4            True         skip            5         True          1     0.2060  \n"
     ]
    }
   ],
   "source": [
    "# Sort the DataFrame in ascending order by 'Test Loss'\n",
    "df_sorted_by_test_loss = df_transformed.sort_values(by='Test Loss', ascending=True)\n",
    "\n",
    "# Display the first few rows of the sorted DataFrame to verify\n",
    "print(df_sorted_by_test_loss.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean 'Test Loss' for each bin in batch_size:\n",
      "batch_size_binned\n",
      "(31.999, 64.0]    0.350475\n",
      "(64.0, 128.0]     0.412060\n",
      "Name: Test Loss, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_526046/2382006718.py:9: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mean_test_loss_per_bin = df_transformed.groupby(feature + '_binned')['Test Loss'].mean()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example numeric feature to bin and analyze\n",
    "feature = 'batch_size'\n",
    "\n",
    "# Define bins for the numeric feature. Adjust these based on your specific data range and granularity preference.\n",
    "# Example: Binning 'batch_size' into 4 equally ranged bins\n",
    "df_transformed[feature + '_binned'] = pd.qcut(df_transformed[feature], q=4, duplicates='drop')\n",
    "\n",
    "# Calculate the mean 'Test Loss' for each bin\n",
    "mean_test_loss_per_bin = df_transformed.groupby(feature + '_binned')['Test Loss'].mean()\n",
    "\n",
    "print(f\"Mean 'Test Loss' for each bin in {feature}:\")\n",
    "print(mean_test_loss_per_bin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean 'Test Loss' for each bin in step_size:\n",
      "step_size_binned\n",
      "(0.999, 8.0]    0.233323\n",
      "(8.0, 16.0]     0.269115\n",
      "(16.0, 32.0]    0.294032\n",
      "(32.0, 64.0]    0.886521\n",
      "Name: Test Loss, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_526046/9858062.py:9: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mean_test_loss_per_bin = df_transformed.groupby(feature + '_binned')['Test Loss'].mean()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example numeric feature to bin and analyze\n",
    "feature = 'step_size'\n",
    "\n",
    "# Define bins for the numeric feature. Adjust these based on your specific data range and granularity preference.\n",
    "# Example: Binning 'batch_size' into 4 equally ranged bins\n",
    "df_transformed[feature + '_binned'] = pd.qcut(df_transformed[feature], q=4, duplicates='drop')\n",
    "\n",
    "# Calculate the mean 'Test Loss' for each bin\n",
    "mean_test_loss_per_bin = df_transformed.groupby(feature + '_binned')['Test Loss'].mean()\n",
    "\n",
    "print(f\"Mean 'Test Loss' for each bin in {feature}:\")\n",
    "print(mean_test_loss_per_bin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean 'Test Loss' for each bin in kernel_size:\n",
      "kernel_size_binned\n",
      "(2.999, 5.0]    0.356409\n",
      "(5.0, 7.0]      0.412372\n",
      "Name: Test Loss, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_526046/2779086597.py:9: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mean_test_loss_per_bin = df_transformed.groupby(feature + '_binned')['Test Loss'].mean()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example numeric feature to bin and analyze\n",
    "feature = 'kernel_size'\n",
    "\n",
    "# Define bins for the numeric feature. Adjust these based on your specific data range and granularity preference.\n",
    "# Example: Binning 'batch_size' into 4 equally ranged bins\n",
    "df_transformed[feature + '_binned'] = pd.qcut(df_transformed[feature], q=4, duplicates='drop')\n",
    "\n",
    "# Calculate the mean 'Test Loss' for each bin\n",
    "mean_test_loss_per_bin = df_transformed.groupby(feature + '_binned')['Test Loss'].mean()\n",
    "\n",
    "print(f\"Mean 'Test Loss' for each bin in {feature}:\")\n",
    "print(mean_test_loss_per_bin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
