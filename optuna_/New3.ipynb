{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 882\u001b[0m\n\u001b[1;32m    879\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m    881\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets[:, :, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m])  \u001b[38;5;66;03m# Assuming specific output handling\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    885\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/function.py:277\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m         backward_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mbackward  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    281\u001b[0m         vjp_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mvjp  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# %%\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_scatter import scatter_add, scatter_max\n",
    "from pytorch_wavelets import DWT1DForward, DWT1DInverse\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional\n",
    "from torch_geometric.utils import to_dense_adj, dense_to_sparse\n",
    "from torch.nn import Sequential, LayerNorm, ReLU\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "from torch.nn import Linear, Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch_geometric.utils import add_self_loops, remove_self_loops, softmax\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn import InstanceNorm1d\n",
    "from torch import Tensor\n",
    "\n",
    "import optuna\n",
    "\n",
    "from RevIN import RevIN\n",
    "\n",
    "\n",
    "# %%\n",
    "class Dataset_Custom(Dataset):\n",
    "    def __init__(self, root_path, flag='train', size=None,\n",
    "                 features='M', data_path='ETTh1.csv',\n",
    "                 target='OT', scale=True, timeenc=0, freq='h',step_size=1):\n",
    "        # Initialize with seq_len and pred_len only\n",
    "        if size is None:\n",
    "            self.seq_len = 96  # Default value, can be adjusted\n",
    "            self.pred_len = 96  # Default value, can be adjusted\n",
    "        else:\n",
    "            self.seq_len, self.pred_len = size[:2]\n",
    "            \n",
    "        \n",
    "        \n",
    "        assert flag in ['train', 'test', 'val']\n",
    "        self.set_type = {'train': 0, 'val': 1, 'test': 2}[flag]\n",
    "\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.scale = scale\n",
    "        self.timeenc = timeenc\n",
    "        self.freq = freq\n",
    "        self.step_size = step_size\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.data_path = data_path\n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        df_raw = pd.read_csv(os.path.join(self.root_path, self.data_path))\n",
    "        \n",
    "        cols_data = [col for col in df_raw.columns if col != 'date']\n",
    "        df_data = df_raw[cols_data]\n",
    "        \n",
    "        num_train = int(len(df_raw) * 0.6)\n",
    "        num_test = int(len(df_raw) * 0.2)\n",
    "        border1s = [0, num_train - self.seq_len, len(df_raw) - num_test - self.seq_len]\n",
    "        border2s = [num_train, num_train + (len(df_raw) - num_train - num_test), len(df_raw)]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "        \n",
    "        if self.features == 'M' or self.features == 'MS':\n",
    "            pass\n",
    "        elif self.features == 'S':\n",
    "            cols_data = df_raw.columns[1:]\n",
    "            df_data = df_raw[cols_data]\n",
    "        \n",
    "        \n",
    "        if self.scale:\n",
    "            train_data = df_data.iloc[border1s[0]:border2s[0]].values\n",
    "            self.scaler.fit(train_data)\n",
    "            data = self.scaler.transform(df_data.values)\n",
    "        else:\n",
    "            data = df_data.values\n",
    "        \n",
    "        df_stamp = df_raw[['date']].iloc[border1:border2]\n",
    "        df_stamp['date'] = pd.to_datetime(df_stamp['date'])\n",
    "        if self.timeenc == 0:\n",
    "            df_stamp['month'] = df_stamp['date'].dt.month\n",
    "            df_stamp['day'] = df_stamp['date'].dt.day\n",
    "            df_stamp['weekday'] = df_stamp['date'].dt.weekday\n",
    "            df_stamp['hour'] = df_stamp['date'].dt.hour\n",
    "            data_stamp = df_stamp.drop(['date'], axis=1).values\n",
    "        elif self.timeenc == 1:\n",
    "            # Example for time_features function\n",
    "            data_stamp = self.time_features(df_stamp['date'], freq=self.freq)\n",
    "\n",
    "        self.data_x = data[border1:border2 - self.pred_len]\n",
    "        self.data_y = data[border1 + self.seq_len:border2]\n",
    "        self.data_stamp = data_stamp\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Calculate the actual start index based on the step size\n",
    "        actual_start_index = index * self.step_size\n",
    "\n",
    "        seq_x = self.data_x[actual_start_index:actual_start_index + self.seq_len]\n",
    "        seq_y = self.data_y[actual_start_index:actual_start_index + self.pred_len]\n",
    "        seq_x_mark = self.data_stamp[actual_start_index:actual_start_index + self.seq_len]\n",
    "        seq_y_mark = self.data_stamp[actual_start_index:actual_start_index + self.pred_len]\n",
    "\n",
    "        return torch.tensor(seq_x,  dtype=torch.float32), torch.tensor(seq_y,  dtype=torch.float32), torch.tensor(seq_x_mark,  dtype=torch.float32), torch.tensor(seq_y_mark,  dtype=torch.float32)\n",
    "\n",
    "\n",
    "    #def __len__(self):\n",
    "        #return len(self.data_x) - self.seq_len + 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Adjust the total length to account for the step size\n",
    "        total_steps = (len(self.data_x) - self.seq_len - self.pred_len + 1) // self.step_size\n",
    "        return total_steps\n",
    "\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)\n",
    "\n",
    "\n",
    "class DilatedTCNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, dropout_rate=0.2, dropout_=True, skip_=True):\n",
    "        super(DilatedTCNBlock, self).__init__()\n",
    "        \n",
    "        # Calculate padding based on kernel size and dilation to maintain input length\n",
    "        padding = (dilation * (kernel_size - 1)) // 2\n",
    "        \n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding, dilation=dilation)\n",
    "        self.norm1 = InstanceNorm1d(out_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate) if dropout_ else nn.Identity()\n",
    "\n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=padding, dilation=dilation)\n",
    "        self.norm2 = InstanceNorm1d(out_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate) if dropout_ else nn.Identity()\n",
    "\n",
    "        self.skip_ = skip_\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_original = x.clone()\n",
    "        \n",
    "        # First conv -> norm -> relu -> dropout\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Second conv -> norm -> relu -> dropout\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        if self.skip_:\n",
    "            x = x + x_original\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        if d_model%2 != 0:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)[:,0:-1]\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "\n",
    "class ProjectedPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_input, d_model, dropout=0.1, max_len=5000):\n",
    "        super(ProjectedPositionalEncoding, self).__init__()\n",
    "        self.d_input = d_input\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Projection layer: maps input dimension to model dimension\n",
    "        self.projection = nn.Linear(d_input, d_model)\n",
    "\n",
    "        self.positionalencoding = PositionalEncoding(d_model,dropout, max_len = 5000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply linear projection\n",
    "        x_projected = self.projection(x)\n",
    "        # Add positional encoding\n",
    "        x_pe = x_projected + self.positionalencoding(x)\n",
    "        return x_pe\n",
    "\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model,n_head = 8, d_ff=None, dropout=0.1, activation=\"relu\",d_k = None, d_v = None, res_attention = False):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        d_k = d_model //n_head if d_k is None else d_k\n",
    "        d_v = d_model // n_head if d_v is None else d_v\n",
    "        self.res_attention = res_attention\n",
    "        self.attention = _MultiheadAttention(d_model, n_head, d_k, d_v, attn_dropout = dropout, proj_dropout = dropout, res_attention = res_attention)\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_x, attn = self.attention(\n",
    "            x, x, x\n",
    "        )\n",
    "        x = x + self.dropout(new_x)\n",
    "\n",
    "        y = x = self.norm1(x)\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "\n",
    "        return self.norm2(x + y), attn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # x [B, L, D]\n",
    "        attns = []\n",
    "        if self.conv_layers is not None:\n",
    "            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                x = conv_layer(x)\n",
    "                attns.append(attn)\n",
    "            x, attn = self.attn_layers[-1](x)\n",
    "            attns.append(attn)\n",
    "        else:\n",
    "            for attn_layer in self.attn_layers:\n",
    "                x, attn = attn_layer(x)\n",
    "                attns.append(attn)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x, attns\n",
    "\n",
    "class Transpose(nn.Module):\n",
    "    def __init__(self, *dims, contiguous=False): \n",
    "        super().__init__()\n",
    "        self.dims, self.contiguous = dims, contiguous\n",
    "    def forward(self, x):\n",
    "        if self.contiguous: return x.transpose(*self.dims).contiguous()\n",
    "        else: return x.transpose(*self.dims)\n",
    "\n",
    "# Attention\n",
    "\n",
    "class _MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_k=None, d_v=None, res_attention=False, attn_dropout=0., proj_dropout=0., qkv_bias=True, lsa=False):\n",
    "        \"\"\"Multi Head Attention Layer\n",
    "        Input shape:\n",
    "            Q:       [batch_size (bs) x max_q_len x d_model]\n",
    "            K, V:    [batch_size (bs) x q_len x d_model]\n",
    "            mask:    [q_len x q_len]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        d_k = d_model // n_heads if d_k is None else d_k\n",
    "        d_v = d_model // n_heads if d_v is None else d_v\n",
    "\n",
    "        self.n_heads, self.d_k, self.d_v = n_heads, d_k, d_v\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=qkv_bias)\n",
    "\n",
    "        # Scaled Dot-Product Attention (multiple heads)\n",
    "        self.res_attention = res_attention\n",
    "        self.sdp_attn = _ScaledDotProductAttention(d_model, n_heads, attn_dropout=attn_dropout, res_attention=self.res_attention, lsa=lsa)\n",
    "\n",
    "        # Poject output\n",
    "        self.to_out = nn.Sequential(nn.Linear(n_heads * d_v, d_model), nn.Dropout(proj_dropout))\n",
    "\n",
    "\n",
    "    def forward(self, Q:Tensor, K:Optional[Tensor]=None, V:Optional[Tensor]=None, prev:Optional[Tensor]=None,\n",
    "                key_padding_mask:Optional[Tensor]=None, attn_mask:Optional[Tensor]=None):\n",
    "\n",
    "        bs = Q.size(0)\n",
    "        if K is None: K = Q\n",
    "        if V is None: V = Q\n",
    "\n",
    "        # Linear (+ split in multiple heads)\n",
    "        q_s = self.W_Q(Q).view(bs, -1, self.n_heads, self.d_k).transpose(1,2)       # q_s    : [bs x n_heads x max_q_len x d_k]\n",
    "        k_s = self.W_K(K).view(bs, -1, self.n_heads, self.d_k).permute(0,2,3,1)     # k_s    : [bs x n_heads x d_k x q_len] - transpose(1,2) + transpose(2,3)\n",
    "        v_s = self.W_V(V).view(bs, -1, self.n_heads, self.d_v).transpose(1,2)       # v_s    : [bs x n_heads x q_len x d_v]\n",
    "\n",
    "        # Apply Scaled Dot-Product Attention (multiple heads)\n",
    "        if self.res_attention:\n",
    "            output, attn_weights, attn_scores = self.sdp_attn(q_s, k_s, v_s, prev=prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        else:\n",
    "            output, attn_weights = self.sdp_attn(q_s, k_s, v_s, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        # output: [bs x n_heads x q_len x d_v], attn: [bs x n_heads x q_len x q_len], scores: [bs x n_heads x max_q_len x q_len]\n",
    "\n",
    "        # back to the original inputs dimensions\n",
    "        output = output.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * self.d_v) # output: [bs x q_len x n_heads * d_v]\n",
    "        output = self.to_out(output)\n",
    "\n",
    "        if self.res_attention: return output, attn_weights, attn_scores\n",
    "        else: return output, attn_weights\n",
    "\n",
    "class _ScaledDotProductAttention(nn.Module):\n",
    "    r\"\"\"Scaled Dot-Product Attention module (Attention is all you need by Vaswani et al., 2017) with optional residual attention from previous layer\n",
    "    (Realformer: Transformer likes residual attention by He et al, 2020) and locality self sttention (Vision Transformer for Small-Size Datasets\n",
    "    by Lee et al, 2021)\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, n_heads, attn_dropout=0., res_attention=False, lsa=False):\n",
    "        super().__init__()\n",
    "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
    "        self.res_attention = res_attention\n",
    "        head_dim = d_model // n_heads\n",
    "        self.scale = nn.Parameter(torch.tensor(head_dim ** -0.5), requires_grad=lsa)\n",
    "        self.lsa = lsa\n",
    "\n",
    "    def forward(self, q:Tensor, k:Tensor, v:Tensor, prev:Optional[Tensor]=None, key_padding_mask:Optional[Tensor]=None, attn_mask:Optional[Tensor]=None):\n",
    "        '''\n",
    "        Input shape:\n",
    "            q               : [bs x n_heads x max_q_len x d_k]\n",
    "            k               : [bs x n_heads x d_k x seq_len]\n",
    "            v               : [bs x n_heads x seq_len x d_v]\n",
    "            prev            : [bs x n_heads x q_len x seq_len]\n",
    "            key_padding_mask: [bs x seq_len]\n",
    "            attn_mask       : [1 x seq_len x seq_len]\n",
    "        Output shape:\n",
    "            output:  [bs x n_heads x q_len x d_v]\n",
    "            attn   : [bs x n_heads x q_len x seq_len]\n",
    "            scores : [bs x n_heads x q_len x seq_len]\n",
    "        '''\n",
    "\n",
    "        # Scaled MatMul (q, k) - similarity scores for all pairs of positions in an input sequence\n",
    "        attn_scores = torch.matmul(q, k) * self.scale      # attn_scores : [bs x n_heads x max_q_len x q_len]\n",
    "\n",
    "        # Add pre-softmax attention scores from the previous layer (optional)\n",
    "        if prev is not None: attn_scores = attn_scores + prev\n",
    "\n",
    "        # Attention mask (optional)\n",
    "        if attn_mask is not None:                                     # attn_mask with shape [q_len x seq_len] - only used when q_len == seq_len\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                attn_scores.masked_fill_(attn_mask, -np.inf)\n",
    "            else:\n",
    "                attn_scores += attn_mask\n",
    "\n",
    "        # Key padding mask (optional)\n",
    "        if key_padding_mask is not None:                              # mask with shape [bs x q_len] (only when max_w_len == q_len)\n",
    "            attn_scores.masked_fill_(key_padding_mask.unsqueeze(1).unsqueeze(2), -np.inf)\n",
    "\n",
    "        # normalize the attention weights\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)                 # attn_weights   : [bs x n_heads x max_q_len x q_len]\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "        # compute the new values given the attention weights\n",
    "        output = torch.matmul(attn_weights, v)                        # output: [bs x n_heads x max_q_len x d_v]\n",
    "\n",
    "        if self.res_attention: return output, attn_weights, attn_scores\n",
    "        else: return output, attn_weights\n",
    "        \n",
    "def Coord1dPosEncoding(q_len, exponential=False, normalize=True):\n",
    "    cpe = (2 * (torch.linspace(0, 1, q_len).reshape(-1, 1)**(.5 if exponential else 1)) - 1)\n",
    "    if normalize:\n",
    "        cpe = cpe - cpe.mean()\n",
    "        cpe = cpe / (cpe.std() * 10)\n",
    "    return cpe\n",
    "\n",
    "\n",
    "def Coord2dPosEncoding(q_len, d_model, exponential=False, normalize=True, eps=1e-3, verbose=False):\n",
    "    x = .5 if exponential else 1\n",
    "    i = 0\n",
    "    for i in range(100):\n",
    "        cpe = 2 * (torch.linspace(0, 1, q_len).reshape(-1, 1) ** x) * (torch.linspace(0, 1, d_model).reshape(1, -1) ** x) - 1\n",
    "        pv(f'{i:4.0f}  {x:5.3f}  {cpe.mean():+6.3f}', verbose)\n",
    "        if abs(cpe.mean()) <= eps: break\n",
    "        elif cpe.mean() > eps: x += .001\n",
    "        else: x -= .001\n",
    "        i += 1\n",
    "    if normalize:\n",
    "        cpe = cpe - cpe.mean()\n",
    "        cpe = cpe / (cpe.std() * 10)\n",
    "    return cpe\n",
    "\n",
    "def positional_encoding(pe, learn_pe, q_len, d_model):\n",
    "    # Positional encoding\n",
    "    if pe == None:\n",
    "        W_pos = torch.empty((q_len, d_model)) # pe = None and learn_pe = False can be used to measure impact of pe\n",
    "        nn.init.uniform_(W_pos, -0.02, 0.02)\n",
    "        learn_pe = False\n",
    "    elif pe == 'zero':\n",
    "        W_pos = torch.empty((q_len, 1))\n",
    "        nn.init.uniform_(W_pos, -0.02, 0.02)\n",
    "    elif pe == 'zeros':\n",
    "        W_pos = torch.empty((q_len, d_model))\n",
    "        nn.init.uniform_(W_pos, -0.02, 0.02)\n",
    "    elif pe == 'normal' or pe == 'gauss':\n",
    "        W_pos = torch.zeros((q_len, 1))\n",
    "        torch.nn.init.normal_(W_pos, mean=0.0, std=0.1)\n",
    "    elif pe == 'uniform':\n",
    "        W_pos = torch.zeros((q_len, 1))\n",
    "        nn.init.uniform_(W_pos, a=0.0, b=0.1)\n",
    "    elif pe == 'lin1d': W_pos = Coord1dPosEncoding(q_len, exponential=False, normalize=True)\n",
    "    elif pe == 'exp1d': W_pos = Coord1dPosEncoding(q_len, exponential=True, normalize=True)\n",
    "    elif pe == 'lin2d': W_pos = Coord2dPosEncoding(q_len, d_model, exponential=False, normalize=True)\n",
    "    elif pe == 'exp2d': W_pos = Coord2dPosEncoding(q_len, d_model, exponential=True, normalize=True)\n",
    "    elif pe == 'sincos': W_pos = PositionalEncoding(q_len, d_model, normalize=True)\n",
    "    else: raise ValueError(f\"{pe} is not a valid pe (positional encoder. Available types: 'gauss'=='normal', \\\n",
    "        'zeros', 'zero', uniform', 'lin1d', 'exp1d', 'lin2d', 'exp2d', 'sincos', None.)\")\n",
    "    return nn.Parameter(W_pos, requires_grad=learn_pe)     \n",
    "\n",
    "class TSTEncoder(nn.Module):\n",
    "    def __init__(self, q_len, d_model, n_heads, d_k=None, d_v=None, d_ff=None, \n",
    "                        norm='BatchNorm', attn_dropout=0., dropout=0., activation='gelu',\n",
    "                        res_attention=False, n_layers=1, pre_norm=False, store_attn=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([TSTEncoderLayer(q_len, d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm,\n",
    "                                                      attn_dropout=attn_dropout, dropout=dropout,\n",
    "                                                      activation=activation, res_attention=res_attention,\n",
    "                                                      pre_norm=pre_norm, store_attn=store_attn) for i in range(n_layers)])\n",
    "        self.res_attention = res_attention\n",
    "\n",
    "    def forward(self, src:Tensor, key_padding_mask:Optional[Tensor]=None, attn_mask:Optional[Tensor]=None):\n",
    "        output = src\n",
    "        scores = None\n",
    "        if self.res_attention:\n",
    "            for mod in self.layers: output, scores = mod(output, prev=scores, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "            return output\n",
    "        else:\n",
    "            for mod in self.layers: output = mod(output, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "            return output\n",
    "        \n",
    "             \n",
    "class TSTiEncoder(nn.Module):  #i means channel-independent\n",
    "    def __init__(self, c_in, patch_num, patch_len, max_seq_len=1024,\n",
    "                 n_layers=3, d_model=128, n_heads=16, d_k=None, d_v=None,\n",
    "                 d_ff=256, norm='BatchNorm', attn_dropout=0., dropout=0., act=\"gelu\", store_attn=False,\n",
    "                 key_padding_mask='auto', padding_var=None, attn_mask=None, res_attention=True, pre_norm=False,\n",
    "                 pe='zeros', learn_pe=True, verbose=False, decompose_layer = 1): #**kwargs\n",
    "        \n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_num = patch_num\n",
    "        self.patch_len = patch_len\n",
    "        \n",
    "        # Input encoding\n",
    "        q_len = patch_num\n",
    "    \n",
    "      \n",
    "        self.W_P = nn.Linear(patch_len, d_model)  # Eq 1: projection of feature vectors onto a d-dim vector space\n",
    "        self.seq_len = q_len\n",
    "\n",
    "        # Positional encoding\n",
    "        self.W_pos = positional_encoding(pe, learn_pe, q_len, d_model) #d_model at the last variable\n",
    "\n",
    "        # Residual dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = TSTEncoder(q_len, d_model, n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout, dropout=dropout,\n",
    "                                   pre_norm=pre_norm, activation=act, res_attention=res_attention, n_layers=n_layers, store_attn=store_attn)\n",
    "\n",
    "        \n",
    "    def forward(self, x) -> Tensor:                                              # x: [bs x nvars x patch_len x patch_num]\n",
    "        \n",
    "        n_vars = x.shape[1]\n",
    "     \n",
    "        # Input encoding\n",
    "        x = x.permute(0,1,3,2)                                                   # x: [bs x nvars x patch_num x patch_len]\n",
    "        \n",
    "     \n",
    "        x = self.W_P(x)                                                          # x: [bs x nvars x patch_num x d_model]\n",
    "     \n",
    "        # After Projection torch.Size([64, 325, 22, 325])\n",
    "        u = torch.reshape(x, (x.shape[0]*x.shape[1],x.shape[2],x.shape[3]))      # u: [bs * nvars x patch_num x d_model]\n",
    "      \n",
    "        u = self.dropout(u + self.W_pos)                                         # u: [bs * nvars x patch_num x d_model]\n",
    "\n",
    "        # Encoder\n",
    "        z = self.encoder(u)                                                      # z: [bs * nvars x patch_num x d_model]\n",
    "        z = torch.reshape(z, (-1,n_vars,z.shape[-2],z.shape[-1]))                # z: [bs x nvars x patch_num x d_model]\n",
    "        z = z.permute(0,1,3,2)                                                   # z: [bs x nvars x d_model x patch_num]\n",
    "        \n",
    "        return z    \n",
    "    \n",
    "class TSTEncoderLayer(nn.Module):\n",
    "    def __init__(self, q_len, d_model, n_heads, d_k=None, d_v=None, d_ff=256, store_attn=False,\n",
    "                 norm='BatchNorm', attn_dropout=0, dropout=0., bias=True, activation=\"gelu\", res_attention=False, pre_norm=False):\n",
    "        super().__init__()\n",
    "        assert not d_model%n_heads, f\"d_model ({d_model}) must be divisible by n_heads ({n_heads})\"\n",
    "        d_k = d_model // n_heads if d_k is None else d_k\n",
    "        d_v = d_model // n_heads if d_v is None else d_v\n",
    "\n",
    "        # Multi-Head attention\n",
    "        self.res_attention = res_attention\n",
    "        self.self_attn = _MultiheadAttention(d_model, n_heads, d_k, d_v, attn_dropout=attn_dropout, proj_dropout=dropout, res_attention=res_attention)\n",
    "\n",
    "        # Add & Norm\n",
    "        self.dropout_attn = nn.Dropout(dropout)\n",
    "        if \"batch\" in norm.lower():\n",
    "            self.norm_attn = nn.Sequential(Transpose(1,2), nn.BatchNorm1d(d_model), Transpose(1,2))\n",
    "        else:\n",
    "            self.norm_attn = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Position-wise Feed-Forward\n",
    "        #get_activation_fn(activation)\n",
    "        self.ff = nn.Sequential(nn.Linear(d_model, d_ff, bias=bias),\n",
    "                                nn.GELU(),\n",
    "                                nn.Dropout(dropout),\n",
    "                                nn.Linear(d_ff, d_model, bias=bias))\n",
    "\n",
    "        # Add & Norm\n",
    "        self.dropout_ffn = nn.Dropout(dropout)\n",
    "        if \"batch\" in norm.lower():\n",
    "            self.norm_ffn = nn.Sequential(Transpose(1,2), nn.BatchNorm1d(d_model), Transpose(1,2))\n",
    "        else:\n",
    "            self.norm_ffn = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.pre_norm = pre_norm\n",
    "        self.store_attn = store_attn\n",
    "\n",
    "\n",
    "    def forward(self, src:Tensor, prev:Optional[Tensor]=None, key_padding_mask:Optional[Tensor]=None, attn_mask:Optional[Tensor]=None) -> Tensor:\n",
    "\n",
    "        # Multi-Head attention sublayer\n",
    "        if self.pre_norm:\n",
    "            src = self.norm_attn(src)\n",
    "        ## Multi-Head attention\n",
    "        if self.res_attention:\n",
    "            src2, attn, scores = self.self_attn(src, src, src, prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        else:\n",
    "            src2, attn = self.self_attn(src, src, src, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        if self.store_attn:\n",
    "            self.attn = attn\n",
    "        ## Add & Norm\n",
    "        src = src + self.dropout_attn(src2) # Add: residual connection with residual dropout\n",
    "        if not self.pre_norm:\n",
    "            src = self.norm_attn(src)\n",
    "\n",
    "        # Feed-forward sublayer\n",
    "        if self.pre_norm:\n",
    "            src = self.norm_ffn(src)\n",
    "        ## Position-wise Feed-Forward\n",
    "        src2 = self.ff(src)\n",
    "        ## Add & Norm\n",
    "        src = src + self.dropout_ffn(src2) # Add: residual connection with residual dropout\n",
    "        if not self.pre_norm:\n",
    "            src = self.norm_ffn(src)\n",
    "\n",
    "        if self.res_attention:\n",
    "            return src, scores\n",
    "        else:\n",
    "            return src\n",
    "class Flatten_Head(nn.Module):\n",
    "    def __init__(self, individual, n_vars, nf, target_window, head_dropout=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.individual = individual\n",
    "        self.n_vars = n_vars\n",
    "        \n",
    "        if self.individual:\n",
    "            self.linears = nn.ModuleList()\n",
    "            self.dropouts = nn.ModuleList()\n",
    "            self.flattens = nn.ModuleList()\n",
    "            for i in range(self.n_vars):\n",
    "                self.flattens.append(nn.Flatten(start_dim=-2))\n",
    "                self.linears.append(nn.Linear(nf, target_window))\n",
    "                self.dropouts.append(nn.Dropout(head_dropout))\n",
    "        else:\n",
    "            self.flatten = nn.Flatten(start_dim=-2)\n",
    "            self.linear = nn.Linear(nf, target_window)\n",
    "            self.dropout = nn.Dropout(head_dropout)\n",
    "            \n",
    "    def forward(self, x):                                 # x: [bs x nvars x d_model x patch_num]\n",
    "        if self.individual:\n",
    "            x_out = []\n",
    "            for i in range(self.n_vars):\n",
    "                z = self.flattens[i](x[:,i,:,:])          # z: [bs x d_model * patch_num]\n",
    "                z = self.linears[i](z)                    # z: [bs x target_window]\n",
    "                z = self.dropouts[i](z)\n",
    "                x_out.append(z)\n",
    "            x = torch.stack(x_out, dim=1)                 # x: [bs x nvars x target_window]\n",
    "        else:\n",
    "            x = self.flatten(x)\n",
    "            x = self.linear(x)\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# %%\n",
    "#stride, padding_patch, patch_length\n",
    "class DWT_MLP_Model(nn.Module):\n",
    "    def __init__(self, input_channels, seq_length, pred_length ,patch_len, mlp_hidden_size, output_channels, stride = 4, padding_patch = True,  decompose_layers=3, wave='haar', \n",
    "                 mode='symmetric', nhead=8, d_model=None, num_encoder_layers=3, dropout=0.1, dilation = 2, \n",
    "                 kernel_size = 3, dropout_ = True, skip_ = True, general_skip_= 'skip', Revin_ = True):\n",
    "        super(DWT_MLP_Model, self).__init__()\n",
    "        self.dwt_forward = DWT1DForward(J=decompose_layers, wave=wave, mode=mode)\n",
    "        self.dwt_inverse = DWT1DInverse(wave=wave, mode=mode)\n",
    "        self.seq_len = seq_length\n",
    "        self.pred_len = pred_length\n",
    "        self.dropout = dropout\n",
    "        self.dilation = dilation\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_decoder_layers = num_encoder_layers\n",
    "        self.dropout_TF = dropout_\n",
    "        self.skip_TF = skip_\n",
    "        self.general_skip = general_skip_\n",
    "        self.Revin = Revin_\n",
    "        self.n_head = nhead\n",
    "        self.padding_patch = padding_patch\n",
    "        context_window = seq_length\n",
    "        self.patch_len = patch_len\n",
    "        \n",
    "        \n",
    "        #patch_num = int((context_window - patch_len)/stride + 1)\n",
    "\n",
    "        if self.Revin:\n",
    "            self.revin_layer = RevIN(input_channels, affine=True, subtract_last=False)\n",
    "\n",
    "        \n",
    "        self.stride = stride\n",
    "        if d_model is None:\n",
    "            d_model = output_channels\n",
    "            \n",
    "        #if self.padding_patch: # can be modified to general case\n",
    "            #self.padding_patch_layer = nn.ReplicationPad1d((0, self.stride)) \n",
    "            #patch_num += 1\n",
    "        \n",
    "        # Assuming kernel_size can be used to derive kernel_set for illustration\n",
    "        self.tcn_low = DilatedTCNBlock(input_channels, output_channels, dilation=self.dilation,kernel_size= self.kernel_size, dropout_rate= self.dropout, dropout_ = self.dropout_TF, skip_ = self.skip_TF)\n",
    "        self.tcn_high_list = nn.ModuleList([DilatedTCNBlock(input_channels, output_channels, dropout_rate= self.dropout, kernel_size= self.kernel_size, dilation= self.dilation) for _ in range(decompose_layers)])\n",
    "\n",
    "        \n",
    "        init_patch_num_low =  context_window//(2*decompose_layers)\n",
    " \n",
    "        patch_num_low = int((init_patch_num_low-patch_len)/stride+1)\n",
    "     \n",
    "        patch_num_high_list = []\n",
    "        seq_head_len = []\n",
    "        for dl in range(1,decompose_layers+1):\n",
    "            if dl == 1 and  decompose_layers != 1:\n",
    "                init_patch_num_high = init_patch_num_low*2\n",
    "                patch_num_high_list.append(int((init_patch_num_high-patch_len)/stride+1)+1)\n",
    "                seq_head_len.append(context_window//2)\n",
    "            elif dl == 1 and decompose_layers ==1:\n",
    "                init_patch_num_high = init_patch_num_low\n",
    "                patch_num_high_list.append(int((init_patch_num_high-patch_len)/stride+1)+1)\n",
    "                seq_head_len.append(init_patch_num_low)\n",
    "            elif dl != decompose_layers:\n",
    "                init_patch_num_high = init_patch_num_low//(2*dl)\n",
    "                patch_num_high_list.append(int((init_patch_num_high-patch_len)/stride+1)+1)\n",
    "                seq_head_len.append(context_window//(2*dl))\n",
    "            else:\n",
    "                init_patch_num_high = init_patch_num_low\n",
    "                patch_num_high_list.append(int((init_patch_num_high-patch_len)/stride+1)+1)\n",
    "                seq_head_len.append(init_patch_num_low)\n",
    "        if self.padding_patch:\n",
    "            self.padding_patch_layer_low = nn.ReplicationPad1d((0, self.stride))\n",
    "            patch_num_low += 1\n",
    "\n",
    "            self.padding_patch_layer_high_list = nn.ModuleList([nn.ReplicationPad1d((0,self.stride)) for _ in range(decompose_layers)])\n",
    "            \n",
    "        self.transformer_low = TSTiEncoder(input_channels, patch_num = patch_num_low, patch_len = patch_len, max_seq_len = 5000, n_layers = num_encoder_layers, d_model = d_model, n_heads = self.n_head, decompose_layer = decompose_layers)\n",
    "        self.head_nf_low = d_model * patch_num_low\n",
    "        self.head_nf_high_list = []\n",
    "        for hnl in range(decompose_layers):\n",
    "            self.head_nf_high_list.append(d_model*patch_num_high_list[hnl])\n",
    "    \n",
    "        # Transformer Encoders for high-frequency components, using custom Encoder\n",
    "        self.transformer_high_list = nn.ModuleList(\n",
    "            [TSTiEncoder(input_channels, patch_num = patch_num_high_list[dls], patch_len = patch_len, max_seq_len = 5000, n_layers = num_encoder_layers, d_model = d_model, n_heads = self.n_head, decompose_layer = 1+ dls) \n",
    "             for dls in range(decompose_layers)])\n",
    "        self.individual = False\n",
    "        self.n_vars = input_channels\n",
    "        self.head_low = Flatten_Head(self.individual, self.n_vars, self.head_nf_low, init_patch_num_low,  head_dropout=self.dropout)\n",
    "        #pred_len//(2*decompose_layers)\n",
    "        self.head_high = nn.ModuleList(\n",
    "            [Flatten_Head(self.individual, self.n_vars, self.head_nf_high_list[f_num],seq_head_len[f_num],  head_dropout=self.dropout) \n",
    "             for f_num in range(decompose_layers)])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #(64, 96, 325)\n",
    "        if self.Revin:\n",
    "            x = self.revin_layer(x, 'norm')\n",
    "            x = x.permute(0,2,1)\n",
    "        else:\n",
    "            x = x.permute(0,2,1)\n",
    " \n",
    "        x_low, x_highs = self.dwt_forward(x)\n",
    "      \n",
    "        \n",
    "        # do patching\n",
    "        if self.padding_patch:\n",
    "            z = self.padding_patch_layer_low(x_low)\n",
    "        \n",
    "            z = z.unfold(dimension=-1, size=self.patch_len, step=self.stride)  # z: [bs x nvars x patch_num x patch_len]\n",
    "     \n",
    "            z = z.permute(0,1,3,2) # z: [bs x nvars x patch_len x patch_num]\n",
    "            x_low = z.clone() \n",
    "            \n",
    "            x_highs_list = []\n",
    "            for xh in range(len(x_highs)):\n",
    "                z = self.padding_patch_layer_high_list[xh](x_highs[xh])\n",
    "                z = z.unfold(dimension=-1, size=self.patch_len, step=self.stride)  # z: [bs x nvars x patch_num x patch_len]\n",
    "                z = z.permute(0,1,3,2) # z: [bs x nvars x patch_len x patch_num]\n",
    "                x_highs_list.append(z)\n",
    "        \n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "        #x_low = x_low.permute(0,1,3,2)\n",
    "        x_low = self.transformer_low(x_low)\n",
    "        x_low = self.head_low(x_low)\n",
    "        x_transformed_list = []\n",
    "        for xhls in range(len(x_highs_list)):\n",
    "            x_transformed_list.append(self.head_high[xhls](self.transformer_high_list[xhls](x_highs_list[xhls])))\n",
    "   \n",
    "     \n",
    "        pred_out = self.dwt_inverse((x_low, x_transformed_list))\n",
    "   \n",
    "        if self.Revin:\n",
    "            pred_out = pred_out.permute(0,2,1)\n",
    "            pred_out = self.revin_layer(pred_out, 'denorm')\n",
    "        else:\n",
    "            pred_out = pred_out\n",
    "        \n",
    "        pred_out = pred_out[:, :, :-4] # Do not make predictions for meta features\n",
    "        pred_out = pred_out[:, -self.pred_len:, :]\n",
    "\n",
    "\n",
    "        return pred_out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assuming DWT_MLP_Model is defined elsewhere, along with the necessary imports\n",
    "seq_ = [24*4, 24*4*4, 512]\n",
    "pred_ = 24*4\n",
    "# Define hyperparameter combinations\n",
    "dropout_enabled = True\n",
    "skip_enabled = True\n",
    "revin_type  = True\n",
    "num_encoder_size = 1\n",
    "general_skip_type = 'skip'\n",
    "mlp_hidden = 128\n",
    "k_size = 5\n",
    "s_size = 8\n",
    "decompose_layer_list = [1,2]\n",
    "bs = 64\n",
    "mt = 'zero'\n",
    "wt = 'haar'\n",
    "dilat = 3\n",
    "patch_lens = 4\n",
    "strides = 4\n",
    "pp = True\n",
    "# Define the ranges for the hyperparameters\n",
    "learning_rates = np.logspace(-3, -2, 100)  # Learning rates between 1e-3 and 1e-2\n",
    "dropout_rates = np.linspace(0.0, 0.2, 100)  # Dropout rates between 0 and 0.5\n",
    "weight_decays = np.logspace(-4, -3, 100)  # Weight decays between 1e-4 and 1e-3\n",
    "indices = np.random.choice(range(100), size=18, replace=False)\n",
    "\n",
    "\n",
    "count = 0\n",
    "\n",
    "for sq in seq_:\n",
    "    for dcls in decompose_layer_list:\n",
    "        #lrs = learning_rates[i]\n",
    "        #dr = dropout_rates[i]\n",
    "        #wd = weight_decays[i]\n",
    "        lrs =0.0052230056036904522\n",
    "        dr = 0.10146011891748014\n",
    "        wd = 1.0059977697794999e-04\n",
    "                                            \n",
    "        # Specify the file path\n",
    "        root_path = '/home/choi/Wave_Transformer/optuna_/electricity/'\n",
    "        data_path = 'electricity.csv'\n",
    "        # Size parameters\n",
    "            \n",
    "        #batch_size = bs\n",
    "        # Initialize the custom dataset for training, validation, and testing\n",
    "        train_dataset = Dataset_Custom(root_path=root_path, features= 'M', flag='train', data_path=data_path, step_size =s_size)\n",
    "        val_dataset = Dataset_Custom(root_path=root_path, features= 'M',flag='val', data_path=data_path,step_size = s_size)\n",
    "        test_dataset = Dataset_Custom(root_path=root_path, features= 'M',flag='test', data_path=data_path,step_size = s_size)\n",
    "\n",
    "        # Optionally, initialize the dataset for prediction (if needed)\n",
    "        #pred_dataset = Dataset_Pred(root_path=root_path, flag='pred', size=size, data_path=data_path, inverse=True)\n",
    "\n",
    "        # Example on how to create DataLoaders for PyTorch training (adjust batch_size as needed)\n",
    "        batch_size = bs\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last = True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last = False)\n",
    "        #print(f\"Running experiment with dilations ={dilat}, wave_type = {wt}, mode_type = {mt},num_encoder_size = {num_encoder_size}, mlp_hidden_size = {mlp_hidden},skip_enabled={skip_enabled}, general_skip={general_skip_type}, batch_size = {bs}, step_size = {s_size}, kernel_size = {k_size}, decompose_layer = {decompose_layer} \")\n",
    "        dropout_rate = dr if dropout_enabled else 0.0\n",
    "        # Adjust the model instantiation to include all hyperparameters\n",
    "        model = DWT_MLP_Model(input_channels=321+4, seq_length=sq, pred_length = pred_, patch_len = patch_lens, mlp_hidden_size=mlp_hidden, \n",
    "                            output_channels=321+4, stride = strides, padding_patch= pp, decompose_layers=dcls, \n",
    "                            dropout=dropout_rate, dilation=dilat, \n",
    "                            mode=mt, wave=wt, kernel_size=k_size,\n",
    "                            num_encoder_layers=num_encoder_size, nhead=5, \n",
    "                            dropout_=dropout_enabled,\n",
    "                            skip_=True, general_skip_=general_skip_type, Revin_=revin_type)\n",
    "\n",
    "        # Define criterion and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lrs, \n",
    "                            weight_decay=wd)\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "\n",
    "        # Early stopping parameters\n",
    "        patience = 10\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        num_epochs = 1\n",
    "\n",
    "        # Start the timer\n",
    "        start_time = time.time()\n",
    "\n",
    "        best_model_path = f\"best_model_{count}_{sq}_{num_encoder_size}_{skip_enabled}_{general_skip_type}_{bs}_{dcls}_{k_size}_{s_size}_{mlp_hidden}.pt\"\n",
    "\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "\n",
    "            train_loss = 0.0\n",
    "\n",
    "            for seq_x, seq_y, seq_x_mark, seq_y_mark in train_loader:\n",
    "            \n",
    "                inputs = torch.cat((seq_x, seq_x_mark), dim=-1)\n",
    "    \n",
    "                targets = torch.cat((seq_y, seq_y_mark), dim=-1)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "            \n",
    "                loss = criterion(outputs, targets[:, :, :-4])  # Assuming specific output handling\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            train_loss /= len(train_loader.dataset)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for seq_x, seq_y, seq_x_mark, seq_y_mark in val_loader:\n",
    "                    inputs = torch.cat((seq_x, seq_x_mark), dim=-1)\n",
    "                    targets = torch.cat((seq_y, seq_y_mark), dim=-1)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets[:, :, :-4])  # Same output handling assumption\n",
    "                    val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            val_loss /= len(val_loader.dataset)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                #print(f\"New best model saved at {best_model_path}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        print(f'Total Model Running Time: {total_time:.2f} seconds')\n",
    "        best_model = DWT_MLP_Model(input_channels=321+4, seq_length=sq, pred_length = pred_,  patch_len = patch_lens,  mlp_hidden_size=mlp_hidden, \n",
    "        output_channels=321+4, stride = strides, padding_patch= pp, decompose_layers=dcls, dropout=dropout_rate, dilation=dilat, \n",
    "        mode=mt, wave=wt, kernel_size=k_size,\n",
    "        num_encoder_layers=num_encoder_size, nhead=5, \n",
    "        dropout_=dropout_enabled,\n",
    "        skip_=True, general_skip_=general_skip_type, Revin_=revin_type)\n",
    "        best_model.load_state_dict(torch.load(best_model_path))\n",
    "        # Evaluation on test data\n",
    "        best_model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for seq_x, seq_y, seq_x_mark, seq_y_mark in test_loader:\n",
    "                inputs = torch.cat((seq_x, seq_x_mark), dim=-1)\n",
    "                targets = torch.cat((seq_y, seq_y_mark), dim=-1)\n",
    "                outputs = best_model(inputs)\n",
    "                loss = criterion(outputs, targets[:, :, :-4])  # Assuming specific output handling\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print(f'The {count}th model done.')\n",
    "        count += 1\n",
    "        print(f'Test Loss for configuration: , seq_len : {sq}, decompose_layer : {dcls}, count: {count}: {test_loss:.4f}')\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
