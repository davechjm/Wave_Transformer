{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 984\u001b[0m\n\u001b[1;32m    981\u001b[0m total_x \u001b[38;5;241m=\u001b[39m load_dataset(root_path, data_path, drop_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# Compute autocorrelation matrix\u001b[39;00m\n\u001b[0;32m--> 984\u001b[0m autocorrelation_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_autocorrelation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    985\u001b[0m correlated_features \u001b[38;5;241m=\u001b[39m extract_correlated_features(autocorrelation_matrix)\n\u001b[1;32m    986\u001b[0m seq_len \u001b[38;5;241m=\u001b[39m seq_length \u001b[38;5;66;03m# 24*4*4\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 826\u001b[0m, in \u001b[0;36mcompute_autocorrelation\u001b[0;34m(seq_x)\u001b[0m\n\u001b[1;32m    823\u001b[0m             autocorrelation_matrix[i, j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m  \u001b[38;5;66;03m# Auto-correlation is always 1\u001b[39;00m\n\u001b[1;32m    824\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    825\u001b[0m             \u001b[38;5;66;03m# Compute the absolute value of Pearson correlation coefficient\u001b[39;00m\n\u001b[0;32m--> 826\u001b[0m             autocorrelation_matrix[i, j] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(\u001b[43mpearsonr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_x\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_x\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m autocorrelation_matrix\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4817\u001b[0m, in \u001b[0;36mpearsonr\u001b[0;34m(x, y, alternative, method)\u001b[0m\n\u001b[1;32m   4814\u001b[0m \u001b[38;5;66;03m# As explained in the docstring, the distribution of `r` under the null\u001b[39;00m\n\u001b[1;32m   4815\u001b[0m \u001b[38;5;66;03m# hypothesis is the beta distribution on (-1, 1) with a = b = n/2 - 1.\u001b[39;00m\n\u001b[1;32m   4816\u001b[0m ab \u001b[38;5;241m=\u001b[39m n\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 4817\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m alternative \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwo-sided\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   4819\u001b[0m     prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mdist\u001b[38;5;241m.\u001b[39msf(\u001b[38;5;28mabs\u001b[39m(r))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/stats/_distn_infrastructure.py:851\u001b[0m, in \u001b[0;36mrv_generic.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m--> 851\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfreeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/stats/_distn_infrastructure.py:846\u001b[0m, in \u001b[0;36mrv_generic.freeze\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Freeze the distribution for the given arguments.\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \n\u001b[1;32m    833\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    843\u001b[0m \n\u001b[1;32m    844\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, rv_continuous):\n\u001b[0;32m--> 846\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrv_continuous_frozen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rv_discrete_frozen(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/stats/_distn_infrastructure.py:465\u001b[0m, in \u001b[0;36mrv_frozen.__init__\u001b[0;34m(self, dist, *args, **kwds)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds \u001b[38;5;241m=\u001b[39m kwds\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# create a new instance\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_updated_ctor_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m shapes, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist\u001b[38;5;241m.\u001b[39m_parse_args(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist\u001b[38;5;241m.\u001b[39m_get_support(\u001b[38;5;241m*\u001b[39mshapes)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/stats/_distn_infrastructure.py:1804\u001b[0m, in \u001b[0;36mrv_continuous.__init__\u001b[0;34m(self, momtype, a, b, xtol, badvalue, name, longname, shapes, seed)\u001b[0m\n\u001b[1;32m   1800\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, momtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, a\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, b\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, xtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-14\u001b[39m,\n\u001b[1;32m   1801\u001b[0m              badvalue\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, longname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1802\u001b[0m              shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1804\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1806\u001b[0m     \u001b[38;5;66;03m# save the ctor parameters, cf generic freeze\u001b[39;00m\n\u001b[1;32m   1807\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctor_param \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m   1808\u001b[0m         momtype\u001b[38;5;241m=\u001b[39mmomtype, a\u001b[38;5;241m=\u001b[39ma, b\u001b[38;5;241m=\u001b[39mb, xtol\u001b[38;5;241m=\u001b[39mxtol,\n\u001b[1;32m   1809\u001b[0m         badvalue\u001b[38;5;241m=\u001b[39mbadvalue, name\u001b[38;5;241m=\u001b[39mname, longname\u001b[38;5;241m=\u001b[39mlongname,\n\u001b[1;32m   1810\u001b[0m         shapes\u001b[38;5;241m=\u001b[39mshapes, seed\u001b[38;5;241m=\u001b[39mseed)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/stats/_distn_infrastructure.py:639\u001b[0m, in \u001b[0;36mrv_generic.__init__\u001b[0;34m(self, seed)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    638\u001b[0m \u001b[38;5;66;03m# figure out if _stats signature has 'moments' keyword\u001b[39;00m\n\u001b[0;32m--> 639\u001b[0m sig \u001b[38;5;241m=\u001b[39m \u001b[43m_getfullargspec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stats_has_moments \u001b[38;5;241m=\u001b[39m ((sig\u001b[38;5;241m.\u001b[39mvarkw \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    641\u001b[0m                            (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmoments\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sig\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    642\u001b[0m                            (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmoments\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sig\u001b[38;5;241m.\u001b[39mkwonlyargs))\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_state \u001b[38;5;241m=\u001b[39m check_random_state(seed)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/_lib/_util.py:383\u001b[0m, in \u001b[0;36mgetfullargspec_no_self\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetfullargspec_no_self\u001b[39m(func):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"inspect.getfullargspec replacement using inspect.signature.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    If func is a bound method, do not list the 'self' parameter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    381\u001b[0m \n\u001b[1;32m    382\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 383\u001b[0m     sig \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    385\u001b[0m         p\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m sig\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m [inspect\u001b[38;5;241m.\u001b[39mParameter\u001b[38;5;241m.\u001b[39mPOSITIONAL_OR_KEYWORD,\n\u001b[1;32m    387\u001b[0m                       inspect\u001b[38;5;241m.\u001b[39mParameter\u001b[38;5;241m.\u001b[39mPOSITIONAL_ONLY]\n\u001b[1;32m    388\u001b[0m     ]\n\u001b[1;32m    389\u001b[0m     varargs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    390\u001b[0m         p\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m sig\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m inspect\u001b[38;5;241m.\u001b[39mParameter\u001b[38;5;241m.\u001b[39mVAR_POSITIONAL\n\u001b[1;32m    392\u001b[0m     ]\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:3254\u001b[0m, in \u001b[0;36msignature\u001b[0;34m(obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msignature\u001b[39m(obj, \u001b[38;5;241m*\u001b[39m, follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   3253\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_wrapped\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_wrapped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3255\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:3002\u001b[0m, in \u001b[0;36mSignature.from_callable\u001b[0;34m(cls, obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   2998\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   2999\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_callable\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   3000\u001b[0m                   follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   3001\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3002\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_signature_from_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigcls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3003\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mfollow_wrapper_chains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_wrapped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3004\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:2404\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2401\u001b[0m sig \u001b[38;5;241m=\u001b[39m _get_signature_of(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__func__\u001b[39m)\n\u001b[1;32m   2403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m skip_bound_arg:\n\u001b[0;32m-> 2404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_signature_bound_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43msig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2405\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sig\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:1964\u001b[0m, in \u001b[0;36m_signature_bound_method\u001b[0;34m(sig)\u001b[0m\n\u001b[1;32m   1959\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_signature_bound_method\u001b[39m(sig):\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Private helper to transform signatures for unbound\u001b[39;00m\n\u001b[1;32m   1961\u001b[0m \u001b[38;5;124;03m    functions to bound methods.\u001b[39;00m\n\u001b[1;32m   1962\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1964\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[43msig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m params \u001b[38;5;129;01mor\u001b[39;00m params[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m (_VAR_KEYWORD, _KEYWORD_ONLY):\n\u001b[1;32m   1967\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minvalid method signature\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:3006\u001b[0m, in \u001b[0;36mSignature.parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3001\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[39;00m\n\u001b[1;32m   3002\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _signature_from_callable(obj, sigcls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   3003\u001b[0m                                     follow_wrapper_chains\u001b[38;5;241m=\u001b[39mfollow_wrapped,\n\u001b[1;32m   3004\u001b[0m                                     \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mglobals\u001b[39m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlocals\u001b[39m, eval_str\u001b[38;5;241m=\u001b[39meval_str)\n\u001b[0;32m-> 3006\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   3007\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparameters\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   3008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters\n\u001b[1;32m   3010\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   3011\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreturn_annotation\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_scatter import scatter_add, scatter_max\n",
    "from pytorch_wavelets import DWT1DForward, DWT1DInverse\n",
    "import warnings\n",
    "\n",
    "import numbers\n",
    "\n",
    "from torch.nn import Parameter\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional\n",
    "from torch_geometric.utils import to_dense_adj, dense_to_sparse\n",
    "from torch.nn import Sequential, LayerNorm, ReLU\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from torch.nn import Linear, Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch_geometric.utils import add_self_loops, remove_self_loops, softmax\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn import InstanceNorm1d\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as init\n",
    "\n",
    "import optuna\n",
    "\n",
    "from RevIN import RevIN\n",
    "\n",
    "def load_dataset(root_path, data_path, drop_columns='date'):\n",
    "    \"\"\"\n",
    "    Load dataset from a CSV file, drop specified columns, and apply standard scaling.\n",
    "\n",
    "    Args:\n",
    "    root_path (str): Directory path where the data file is located.\n",
    "    data_path (str): Name of the data file.\n",
    "    drop_columns (list of str): Columns to be dropped from the dataset.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Scaled dataset as a NumPy array.\n",
    "    \"\"\"\n",
    "    # Combine the root path and the data path\n",
    "    full_path = os.path.join(root_path, data_path)\n",
    "    \n",
    "    # Read the data from CSV file\n",
    "    data = pd.read_csv(full_path)\n",
    "    \n",
    "    # Drop specified columns if provided\n",
    "    if drop_columns:\n",
    "        data.drop(columns=drop_columns, inplace=True, errors='ignore')\n",
    "    \n",
    "    # Apply StandardScaler to normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data.values)  # Assuming data to be normalized is numeric\n",
    "\n",
    "    return scaled_data\n",
    "\n",
    "    \n",
    "class Dataset_Custom(Dataset):\n",
    "    def __init__(self, root_path, flag='train', size=None,\n",
    "                 features='M', data_path='ETTh1.csv',\n",
    "                 target='OT', scale=True, timeenc=0, freq='h',step_size=1):\n",
    "        # Initialize with seq_len and pred_len only\n",
    "        if size is None:\n",
    "            self.seq_len = 96  # Default value, can be adjusted\n",
    "            self.pred_len = 96  # Default value, can be adjusted\n",
    "        else:\n",
    "            self.seq_len, self.pred_len = size[:2]\n",
    "            \n",
    "        \n",
    "        \n",
    "        assert flag in ['train', 'test', 'val']\n",
    "        self.set_type = {'train': 0, 'val': 1, 'test': 2}[flag]\n",
    "\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.scale = scale\n",
    "        self.timeenc = timeenc\n",
    "        self.freq = freq\n",
    "        self.step_size = step_size\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.data_path = data_path\n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        df_raw = pd.read_csv(os.path.join(self.root_path, self.data_path))\n",
    "        \n",
    "        cols_data = [col for col in df_raw.columns if col != 'date']\n",
    "        df_data = df_raw[cols_data]\n",
    "        \n",
    "        num_train = int(len(df_raw) * 0.6)\n",
    "        num_test = int(len(df_raw) * 0.2)\n",
    "        border1s = [0, num_train - self.seq_len, len(df_raw) - num_test - self.seq_len]\n",
    "        border2s = [num_train, num_train + (len(df_raw) - num_train - num_test), len(df_raw)]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "        \n",
    "        if self.features == 'M' or self.features == 'MS':\n",
    "            pass\n",
    "        elif self.features == 'S':\n",
    "            cols_data = df_raw.columns[1:]\n",
    "            df_data = df_raw[cols_data]\n",
    "        \n",
    "        \n",
    "        if self.scale:\n",
    "            train_data = df_data.iloc[border1s[0]:border2s[0]].values\n",
    "            self.scaler.fit(train_data)\n",
    "            data = self.scaler.transform(df_data.values)\n",
    "        else:\n",
    "            data = df_data.values\n",
    "        \n",
    "        df_stamp = df_raw[['date']].iloc[border1:border2]\n",
    "        df_stamp['date'] = pd.to_datetime(df_stamp['date'])\n",
    "        if self.timeenc == 0:\n",
    "            df_stamp['month'] = df_stamp['date'].dt.month\n",
    "            df_stamp['day'] = df_stamp['date'].dt.day\n",
    "            df_stamp['weekday'] = df_stamp['date'].dt.weekday\n",
    "            df_stamp['hour'] = df_stamp['date'].dt.hour\n",
    "            data_stamp = df_stamp.drop(['date'], axis=1).values\n",
    "        elif self.timeenc == 1:\n",
    "            # Example for time_features function\n",
    "            data_stamp = self.time_features(df_stamp['date'], freq=self.freq)\n",
    "\n",
    "        self.data_x = data[border1:border2 - self.pred_len]\n",
    "        self.data_y = data[border1 + self.seq_len:border2]\n",
    "        self.data_stamp = data_stamp\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Calculate the actual start index based on the step size\n",
    "        actual_start_index = index * self.step_size\n",
    "\n",
    "        seq_x = self.data_x[actual_start_index:actual_start_index + self.seq_len]\n",
    "        seq_y = self.data_y[actual_start_index:actual_start_index + self.pred_len]\n",
    "        seq_x_mark = self.data_stamp[actual_start_index:actual_start_index + self.seq_len]\n",
    "        seq_y_mark = self.data_stamp[actual_start_index:actual_start_index + self.pred_len]\n",
    "\n",
    "        return torch.tensor(seq_x,  dtype=torch.float32), torch.tensor(seq_y,  dtype=torch.float32), torch.tensor(seq_x_mark,  dtype=torch.float32), torch.tensor(seq_y_mark,  dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Adjust the total length to account for the step size\n",
    "        total_steps = (len(self.data_x) - self.seq_len - self.pred_len + 1) // self.step_size\n",
    "        return total_steps\n",
    "\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)\n",
    "\n",
    "\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, c_in: int, c_out: int, bias: bool = True):\n",
    "        super(Linear, self).__init__()\n",
    "        self._mlp = torch.nn.Conv2d(\n",
    "            c_in, c_out, kernel_size=(1, 1), padding=(0, 0), stride=(1, 1), bias=bias\n",
    "        )\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "            else:\n",
    "                nn.init.uniform_(p)\n",
    "\n",
    "    def forward(self, X: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        return self._mlp(X)\n",
    "\n",
    "\n",
    "class MixProp(nn.Module):\n",
    "    def __init__(self, c_in: int, c_out: int, gdep: int, dropout: float, alpha: float):\n",
    "        super(MixProp, self).__init__()\n",
    "        self._mlp = Linear((gdep + 1) * c_in, c_out)\n",
    "        self._gdep = gdep\n",
    "        self._dropout = dropout\n",
    "        self._alpha = alpha\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "            else:\n",
    "                nn.init.uniform_(p)\n",
    "\n",
    "    def forward(self, X: torch.FloatTensor, A: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        A = A + torch.eye(A.size(0)).to(X.device)\n",
    "        d = A.sum(1)\n",
    "        H = X\n",
    "        H_0 = X\n",
    "        A = A / d.view(-1, 1)\n",
    "        for _ in range(self._gdep):\n",
    "            \n",
    "            H = self._alpha * X + (1 - self._alpha) * torch.einsum(\n",
    "                \"ncwl,vw->ncvl\", (H, A)\n",
    "            )\n",
    "            H_0 = torch.cat((H_0, H), dim=1)\n",
    "        H_0 = self._mlp(H_0)\n",
    "        return H_0\n",
    "\n",
    "\n",
    "class DilatedInception(nn.Module):\n",
    "    def __init__(self, c_in: int, c_out: int, kernel_set: list, dilation_factor: int):\n",
    "        super(DilatedInception, self).__init__()\n",
    "        self._time_conv = nn.ModuleList()\n",
    "        self._kernel_set = kernel_set\n",
    "        c_out = int(c_out / len(self._kernel_set))\n",
    "        for kern in self._kernel_set:\n",
    "            self._time_conv.append(\n",
    "                nn.Conv2d(c_in, c_out, (1, kern), dilation=(1, dilation_factor))\n",
    "            )\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "            else:\n",
    "                nn.init.uniform_(p)\n",
    "\n",
    "    def forward(self, X_in: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \n",
    "        X = []\n",
    "        for i in range(len(self._kernel_set)):\n",
    "            X.append(self._time_conv[i](X_in))\n",
    "        \n",
    "        for i in range(len(self._kernel_set)):\n",
    "            X[i] = X[i][..., -X[-1].size(3) :]\n",
    "        \n",
    "        \n",
    "        Y = [0 ,0 ,0, 0]\n",
    "        for i in range(len(self._kernel_set)):\n",
    "            Y[i] = X[i].permute(0, 2, 1, 3)\n",
    "        \n",
    "        Y = torch.cat(Y, dim=2)\n",
    "        \n",
    "        X = Y.permute(0, 2, 1, 3)\n",
    "        return  X\n",
    "\n",
    "\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    __constants__ = [\"normalized_shape\", \"weight\", \"bias\", \"eps\", \"elementwise_affine\"]\n",
    "\n",
    "    def __init__(\n",
    "        self, normalized_shape: int, eps: float = 1e-5, elementwise_affine: bool = True\n",
    "    ):\n",
    "        super(LayerNormalization, self).__init__()\n",
    "        self._normalized_shape = tuple(normalized_shape)\n",
    "        self._eps = eps\n",
    "        self._elementwise_affine = elementwise_affine\n",
    "        if self._elementwise_affine:\n",
    "            self._weight = nn.Parameter(torch.Tensor(*normalized_shape))\n",
    "            self._bias = nn.Parameter(torch.Tensor(*normalized_shape))\n",
    "        else:\n",
    "            self.register_parameter(\"_weight\", None)\n",
    "            self.register_parameter(\"_bias\", None)\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        if self._elementwise_affine:\n",
    "            init.ones_(self._weight)\n",
    "            init.zeros_(self._bias)\n",
    "\n",
    "    def forward(self, X: torch.FloatTensor, idx: torch.LongTensor) -> torch.FloatTensor:\n",
    "        if self._elementwise_affine:\n",
    "            return F.layer_norm(\n",
    "                X,\n",
    "                tuple(X.shape[1:]),\n",
    "                self._weight[:, idx, :],\n",
    "                self._bias[:, idx, :],\n",
    "                self._eps,\n",
    "            )\n",
    "        else:\n",
    "            return F.layer_norm(\n",
    "                X, tuple(X.shape[1:]), self._weight, self._bias, self._eps\n",
    "            )\n",
    "\n",
    "\n",
    "class GPModuleLayer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dilation_exponential: int,\n",
    "        rf_size_i: int,\n",
    "        kernel_size: int,\n",
    "        j: int,\n",
    "        residual_channels: int,\n",
    "        conv_channels: int,\n",
    "        skip_channels: int,\n",
    "        kernel_set: list,\n",
    "        new_dilation: int,\n",
    "        layer_norm_affline: bool,\n",
    "        gcn_true: bool,\n",
    "        seq_length: int,\n",
    "        receptive_field: int,\n",
    "        dropout: float,\n",
    "        gcn_depth: int,\n",
    "        num_nodes: int,\n",
    "        propalpha: float,\n",
    "    ):\n",
    "        super(GPModuleLayer, self).__init__()\n",
    "        self._dropout = dropout\n",
    "        self._gcn_true = gcn_true\n",
    "        \n",
    "        if dilation_exponential > 1:\n",
    "            rf_size_j = int(\n",
    "                rf_size_i\n",
    "                + (kernel_size - 1)\n",
    "                * (dilation_exponential ** j - 1)\n",
    "                / (dilation_exponential - 1)\n",
    "            )\n",
    "        else:\n",
    "            rf_size_j = rf_size_i + j * (kernel_size - 1)\n",
    "        \n",
    "        self._filter_conv = DilatedInception(\n",
    "            residual_channels,\n",
    "            conv_channels,\n",
    "            kernel_set=kernel_set,\n",
    "            dilation_factor=new_dilation,\n",
    "        )\n",
    "        \n",
    "        self._gate_conv = DilatedInception(\n",
    "            residual_channels,\n",
    "            conv_channels,\n",
    "            kernel_set=kernel_set,\n",
    "            dilation_factor=new_dilation,\n",
    "        )\n",
    "\n",
    "        self._residual_conv = nn.Conv2d(\n",
    "            in_channels=conv_channels,\n",
    "            out_channels=residual_channels,\n",
    "            kernel_size=(1, 1),\n",
    "        )\n",
    "\n",
    "        if seq_length > receptive_field:\n",
    "            self._skip_conv = nn.Conv2d(\n",
    "                in_channels=conv_channels,\n",
    "                out_channels=skip_channels,\n",
    "                kernel_size=(1, seq_length - rf_size_j + 1),\n",
    "            )\n",
    "        else:\n",
    "            self._skip_conv = nn.Conv2d(\n",
    "                in_channels=conv_channels,\n",
    "                out_channels=skip_channels,\n",
    "                kernel_size=(1, receptive_field - rf_size_j + 1),\n",
    "            )\n",
    "\n",
    "        if gcn_true:\n",
    "            self._mixprop_conv1 = MixProp(\n",
    "                conv_channels, residual_channels, gcn_depth, dropout, propalpha\n",
    "            )\n",
    "\n",
    "            self._mixprop_conv2 = MixProp(\n",
    "                conv_channels, residual_channels, gcn_depth, dropout, propalpha\n",
    "            )\n",
    "\n",
    "        if seq_length > receptive_field:\n",
    "            self._normalization = LayerNormalization(\n",
    "                (residual_channels, num_nodes, seq_length - rf_size_j + 1),\n",
    "                elementwise_affine=layer_norm_affline,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self._normalization = LayerNormalization(\n",
    "                (residual_channels, num_nodes, receptive_field - rf_size_j + 1),\n",
    "                elementwise_affine=layer_norm_affline,\n",
    "            )\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "            else:\n",
    "                nn.init.uniform_(p)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        X: torch.FloatTensor,\n",
    "        X_skip: torch.FloatTensor,\n",
    "        A_tilde: Optional[torch.FloatTensor],\n",
    "        idx: torch.LongTensor,\n",
    "        training: bool,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \n",
    "        X_residual = X\n",
    "        X_filter = self._filter_conv(X)\n",
    "        X_filter = torch.tanh(X_filter)\n",
    "        X_gate = self._gate_conv(X)\n",
    "        X_gate = torch.sigmoid(X_gate)\n",
    "        X = X_filter * X_gate\n",
    "        X = F.dropout(X, self._dropout, training=training)\n",
    "        X_skip = self._skip_conv(X) + X_skip\n",
    "        if self._gcn_true:\n",
    "            X = self._mixprop_conv1(X, A_tilde) + self._mixprop_conv2(\n",
    "                X, A_tilde.transpose(1, 0)\n",
    "            )\n",
    "        else:\n",
    "            X = self._residual_conv(X)\n",
    "\n",
    "        X = X + X_residual[:, :, :, -X.size(3) :]\n",
    "        X = self._normalization(X, idx)\n",
    "        return X, X_skip\n",
    "\n",
    "\n",
    "class GPModule(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        gcn_true: bool,\n",
    "        build_adj: bool,\n",
    "        gcn_depth: int,\n",
    "        num_nodes: int,\n",
    "        kernel_set: list,\n",
    "        kernel_size: int,\n",
    "        dropout: float,\n",
    "        dilation_exponential: int,\n",
    "        conv_channels: int,\n",
    "        residual_channels: int,\n",
    "        skip_channels: int,\n",
    "        end_channels: int,\n",
    "        seq_length: int,\n",
    "        in_dim: int,\n",
    "        out_dim: int,\n",
    "        layers: int,\n",
    "        propalpha: float,\n",
    "        layer_norm_affline: bool,\n",
    "        graph_constructor,\n",
    "        xd: Optional[int] = None,\n",
    "    ):\n",
    "        super(GPModule, self).__init__()\n",
    "        \n",
    "        self._gcn_true = gcn_true\n",
    "        self._build_adj_true = build_adj\n",
    "        self._num_nodes = num_nodes\n",
    "        self._dropout = dropout\n",
    "        self._seq_length = seq_length\n",
    "        self._layers = layers\n",
    "        self._idx = torch.arange(self._num_nodes)\n",
    "        \n",
    "        self._gp_layers = nn.ModuleList()\n",
    "        \n",
    "        self._graph_constructor = graph_constructor\n",
    "        \n",
    "        self._set_receptive_field(dilation_exponential, kernel_size, layers)\n",
    "        \n",
    "        new_dilation = 1\n",
    "        for j in range(1, layers + 1):\n",
    "            self._gp_layers.append(\n",
    "                GPModuleLayer(\n",
    "                    dilation_exponential=dilation_exponential,\n",
    "                    rf_size_i=1,\n",
    "                    kernel_size=kernel_size,\n",
    "                    j=j,\n",
    "                    residual_channels=residual_channels,\n",
    "                    conv_channels=conv_channels,\n",
    "                    skip_channels=skip_channels,\n",
    "                    kernel_set=kernel_set,\n",
    "                    new_dilation=new_dilation,\n",
    "                    layer_norm_affline=layer_norm_affline,\n",
    "                    gcn_true=gcn_true,\n",
    "                    seq_length=seq_length,\n",
    "                    receptive_field=self._receptive_field,\n",
    "                    dropout=dropout,\n",
    "                    gcn_depth=gcn_depth,\n",
    "                    num_nodes=num_nodes,\n",
    "                    propalpha=propalpha,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            new_dilation *= dilation_exponential\n",
    "        \n",
    "        self._setup_conv(\n",
    "            in_dim, skip_channels, end_channels, residual_channels, out_dim\n",
    "        )\n",
    "        \n",
    "        self._reset_parameters()\n",
    "        \n",
    "    def _setup_conv(\n",
    "        self, in_dim, skip_channels, end_channels, residual_channels, out_dim\n",
    "    ):\n",
    "    \n",
    "        self._start_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=residual_channels, kernel_size=(1, 1)\n",
    "        )\n",
    "        \n",
    "        if self._seq_length > self._receptive_field:\n",
    "            \n",
    "            self._skip_conv_0 = nn.Conv2d(\n",
    "                in_channels=in_dim,\n",
    "                out_channels=skip_channels,\n",
    "                kernel_size=(1, self._seq_length),\n",
    "                bias=True,\n",
    "            )\n",
    "            \n",
    "            self._skip_conv_E = nn.Conv2d(\n",
    "                in_channels=residual_channels,\n",
    "                out_channels=skip_channels,\n",
    "                kernel_size=(1, self._seq_length - self._receptive_field + 1),\n",
    "                bias=True,\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            self._skip_conv_0 = nn.Conv2d(\n",
    "                in_channels=in_dim,\n",
    "                out_channels=skip_channels,\n",
    "                kernel_size=(1, self._receptive_field),\n",
    "                bias=True,\n",
    "            )\n",
    "            \n",
    "            self._skip_conv_E = nn.Conv2d(\n",
    "                in_channels=residual_channels,\n",
    "                out_channels=skip_channels,\n",
    "                kernel_size=(1, 1),\n",
    "                bias=True,\n",
    "            )\n",
    "        \n",
    "        self._end_conv_1 = nn.Conv2d(\n",
    "            in_channels=skip_channels,\n",
    "            out_channels=end_channels,\n",
    "            kernel_size=(1, 1),\n",
    "            bias=True,\n",
    "        )\n",
    "        \n",
    "        self._end_conv_2 = nn.Conv2d(\n",
    "            in_channels=end_channels,\n",
    "            out_channels=out_dim,\n",
    "            kernel_size=(1, 1),\n",
    "            bias=True,\n",
    "        )\n",
    "    \n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "            else:\n",
    "                nn.init.uniform_(p)\n",
    "    \n",
    "    def _set_receptive_field(self, dilation_exponential, kernel_size, layers):\n",
    "        if dilation_exponential > 1:\n",
    "            self._receptive_field = int(\n",
    "                1\n",
    "                + (kernel_size - 1)\n",
    "                * (dilation_exponential ** layers - 1)\n",
    "                / (dilation_exponential - 1)\n",
    "            )\n",
    "        else:\n",
    "            self._receptive_field = layers * (kernel_size - 1) + 1\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        context: torch.FloatTensor,\n",
    "        A_tilde: Optional[torch.FloatTensor] = None,\n",
    "        idx: Optional[torch.LongTensor] = None,\n",
    "        FE: Optional[torch.FloatTensor] = None,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \n",
    "        X_in = context.permute(0, 3, 2, 1)\n",
    "        \n",
    "        seq_len = X_in.size(3)\n",
    "        assert (\n",
    "            seq_len == self._seq_length\n",
    "        ), \"Input sequence length not equal to preset sequence length.\"\n",
    "        \n",
    "        if self._seq_length < self._receptive_field:\n",
    "            X_in = nn.functional.pad(\n",
    "                X_in, (self._receptive_field - self._seq_length, 0, 0, 0)\n",
    "            )\n",
    "        \n",
    "        if self._gcn_true:\n",
    "            if self._build_adj_true:\n",
    "                if idx is None:\n",
    "                    A_tilde = self._graph_constructor(self._idx.to(X_in.device), FE=FE)\n",
    "                else:\n",
    "                    A_tilde = self._graph_constructor(idx, FE=FE)\n",
    "        \n",
    "        X = self._start_conv(X_in)\n",
    "        X_skip = self._skip_conv_0(\n",
    "            F.dropout(X_in, self._dropout, training=self.training)\n",
    "        )\n",
    "        if idx is None:\n",
    "            for gp in self._gp_layers:\n",
    "                \n",
    "                X, X_skip = gp(X, X_skip, A_tilde, self._idx.to(X_in.device), self.training)\n",
    "        else:\n",
    "            for gp in self._gp_layers:\n",
    "                X, X_skip = gp(X, X_skip, A_tilde, idx, self.training)\n",
    "        \n",
    "        X_skip = self._skip_conv_E(X) + X_skip\n",
    "        X = F.relu(X_skip)\n",
    "        X = F.relu(self._end_conv_1(X))\n",
    "        X = self._end_conv_2(X)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    \n",
    "    \n",
    "    def load_my_state_dict(self, state_dict):\n",
    "        own_state = self.state_dict()\n",
    "        for name, param in state_dict.items():\n",
    "            if isinstance(param, Parameter):\n",
    "                param = param.data\n",
    "            try:\n",
    "                own_state[name].copy_(param)\n",
    "            except:\n",
    "                print(name)\n",
    "                print(param.shape)\n",
    "\n",
    "\n",
    "class GraphConstructor(nn.Module):\n",
    "    def __init__(\n",
    "        self, nnodes: int, k: int, dim: int, alpha: float, xd: Optional[int] = None\n",
    "    ):\n",
    "        super(GraphConstructor, self).__init__()\n",
    "        if xd is not None:\n",
    "            self._static_feature_dim = xd\n",
    "            self._linear1 = nn.Linear(xd, dim)\n",
    "            self._linear2 = nn.Linear(xd, dim)\n",
    "        else:\n",
    "            self._embedding1 = nn.Embedding(nnodes, dim)\n",
    "            self._embedding2 = nn.Embedding(nnodes, dim)\n",
    "            self._linear1 = nn.Linear(dim, dim)\n",
    "            self._linear2 = nn.Linear(dim, dim)\n",
    "        \n",
    "        self._k = k\n",
    "        self._alpha = alpha\n",
    "        \n",
    "        self._reset_parameters()\n",
    "        \n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "            else:\n",
    "                nn.init.uniform_(p)\n",
    "\n",
    "    def forward(\n",
    "        self, idx: torch.LongTensor, FE: Optional[torch.FloatTensor] = None\n",
    "    ) -> torch.FloatTensor:\n",
    "        \n",
    "        if FE is None:\n",
    "            nodevec1 = self._embedding1(idx)\n",
    "            nodevec2 = self._embedding2(idx)\n",
    "        else:\n",
    "            assert FE.shape[1] == self._static_feature_dim\n",
    "            nodevec1 = FE[idx, :]\n",
    "            nodevec2 = nodevec1 \n",
    "        \n",
    "        nodevec1 = torch.tanh(self._alpha * self._linear1(nodevec1))\n",
    "        nodevec2 = torch.tanh(self._alpha * self._linear2(nodevec2))\n",
    "    \n",
    "        a = torch.mm(nodevec1, nodevec2.transpose(1, 0)) - torch.mm(\n",
    "            nodevec2, nodevec1.transpose(1, 0)\n",
    "        )\n",
    "        A = F.relu(torch.tanh(self._alpha * a))\n",
    "        mask = torch.zeros(idx.size(0), idx.size(0)).to(A.device)\n",
    "        mask.fill_(float(\"0\"))\n",
    "        \n",
    "        s1, t1 = A.topk(self._k, 1)\n",
    "        mask.scatter_(1, t1, s1.fill_(1))\n",
    "        A = A * mask\n",
    "        return A\n",
    "\n",
    "class Model_(nn.Module):\n",
    "    def __init__(self, seq_len, pred_len, n_points, dropout, wavelet_j, wavelet, subgraph_size, node_dim, n_gnn_layer):\n",
    "        super(Model_, self).__init__()\n",
    "        self.seq_len = seq_len # Sequence Length\n",
    "        self.pred_len = pred_len # Pred Length\n",
    "        self.points = n_points # Num Features\n",
    "        self.dropout = dropout #dropout rate\n",
    "        \n",
    "        decompose_layer = wavelet_j # the number of wavelet decompose layer\n",
    "        wave = wavelet # wavelet function e.g.) 'haar'\n",
    "        \n",
    "        mode = 'symmetric' \n",
    "        self.dwt = DWT1DForward(wave=wave, J=decompose_layer, mode=mode)  \n",
    "        self.idwt = DWT1DInverse(wave=wave)\n",
    "        \n",
    "        \n",
    "        tmp1 = torch.randn(1, 1, self.seq_len)\n",
    "        tmp1_yl, tmp1_yh = self.dwt(tmp1)\n",
    "        tmp1_coefs = [tmp1_yl] + tmp1_yh\n",
    "        \n",
    "        tmp2 = torch.randn(1, 1, self.seq_len + self.pred_len)\n",
    "        tmp2_yl, tmp2_yh = self.dwt(tmp2)\n",
    "        tmp2_coefs = [tmp2_yl] + tmp2_yh\n",
    "        assert decompose_layer + 1 == len(tmp1_coefs) == len(tmp2_coefs)\n",
    "        \n",
    "        self._graph_constructor = GraphConstructor(\n",
    "            nnodes=self.points,\n",
    "            k=subgraph_size, # topk\n",
    "            dim=node_dim, # node_dim in graph\n",
    "            alpha=3.0\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.nets = nn.ModuleList()\n",
    "        for i in range(decompose_layer + 1):\n",
    "            self.nets.append(\n",
    "                GPModule(\n",
    "                    gcn_true = True,\n",
    "                    build_adj = True,\n",
    "                    gcn_depth=2,\n",
    "                    num_nodes=self.points,\n",
    "                    kernel_set=[2, 3, 6, 7],\n",
    "                    kernel_size=7,\n",
    "                    dropout=self.dropout,\n",
    "                    conv_channels=32,\n",
    "                    residual_channels=32,\n",
    "                    skip_channels=64,\n",
    "                    end_channels=128,\n",
    "                    seq_length=(tmp1_coefs[i].shape[-1]),\n",
    "                    in_dim=1,\n",
    "                    out_dim=(tmp2_coefs[i].shape[-1]) - (tmp1_coefs[i].shape[-1]),\n",
    "                    layers=n_gnn_layer, # the nubmer of layers of gnn\n",
    "                    propalpha=0.05,\n",
    "                    dilation_exponential=2,\n",
    "                    graph_constructor=self._graph_constructor,\n",
    "                    layer_norm_affline=True,\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    \n",
    "    def model(self, coefs):\n",
    "        new_coefs = []\n",
    "        for coef, net in zip(coefs, self.nets):\n",
    "            new_coef = net(coef.permute(0,2,1).unsqueeze(-1))\n",
    "            new_coefs.append(new_coef.squeeze().permute(0,2,1))\n",
    "        \n",
    "        return new_coefs\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, x_enc):\n",
    "        in_dwt = x_enc.permute(0,2,1)\n",
    "        \n",
    "        yl, yhs = self.dwt(in_dwt)\n",
    "        coefs = [yl] + yhs\n",
    "        \n",
    "        \n",
    "        coefs_new = self.model(coefs)\n",
    "        \n",
    "        coefs_idwt = []\n",
    "        for i in range(len(coefs_new)):\n",
    "            coefs_idwt.append(torch.cat((coefs[i], coefs_new[i]), 2))\n",
    "        \n",
    "        \n",
    "        out = self.idwt((coefs_idwt[0], coefs_idwt[1:]))\n",
    "        pred_out = out.permute(0, 2, 1)\n",
    "        \n",
    "        \n",
    "        return pred_out[:, -self.pred_len:, :]\n",
    "\n",
    "class moving_avg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "\n",
    "# Define a function to extract correlated features for each feature\n",
    "def extract_correlated_features(autocorrelation_matrix, num_correlated_features=10):\n",
    "    num_features = autocorrelation_matrix.shape[0]\n",
    "    correlated_features = {}\n",
    "    for i in range(num_features):\n",
    "        # Find indices of the top correlated features\n",
    "        correlated_indices = np.argsort(autocorrelation_matrix[i])[::-1][:num_correlated_features]\n",
    "        correlated_features[i] = correlated_indices\n",
    "    return correlated_features   \n",
    "\n",
    "def compute_autocorrelation(seq_x):\n",
    "    num_features = seq_x.shape[1]  # seq_x is now [seq_len, num_channels]\n",
    "    autocorrelation_matrix = np.zeros((num_features, num_features))\n",
    "    \n",
    "    for i in range(num_features):\n",
    "        for j in range(num_features):\n",
    "            if i == j:\n",
    "                autocorrelation_matrix[i, j] = 1.0  # Auto-correlation is always 1\n",
    "            else:\n",
    "                # Compute the absolute value of Pearson correlation coefficient\n",
    "                autocorrelation_matrix[i, j] = np.abs(pearsonr(seq_x[:, i], seq_x[:, j])[0])\n",
    "    \n",
    "    return autocorrelation_matrix\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, seq_len, pred_len, n_points, dropout, wavelet_j, wavelet, subgraph_size, node_dim, n_gnn_layer, correlated_groups):\n",
    "        super(Model, self).__init__()\n",
    "        self.channels = n_points\n",
    "        self.correlated_groups = correlated_groups  # Pass the correlated groups as a parameter\n",
    "        self.backbone_modules = nn.ModuleDict({\n",
    "            str(i): Model_(\n",
    "                seq_len=seq_len,\n",
    "                pred_len=pred_len,\n",
    "                n_points=len(group),\n",
    "                dropout=dropout,\n",
    "                wavelet_j=wavelet_j,\n",
    "                wavelet=wavelet,\n",
    "                subgraph_size=subgraph_size,\n",
    "                node_dim=node_dim,\n",
    "                n_gnn_layer=n_gnn_layer\n",
    "            )\n",
    "            for i, group in enumerate(correlated_groups.values())\n",
    "        })\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.zeros([x.size(0), self.pred_len, x.size(2)], dtype=x.dtype)\n",
    "        # Dictionary to collect predictions for averaging\n",
    "        predictions = {i: [] for i in range(x.size(2))}\n",
    "        \n",
    "        for idx, group in enumerate(self.correlated_groups.values()):\n",
    "            # Extracting the specific features for this group\n",
    "            group_data = x[:, :, group]\n",
    "            pred = self.backbone_modules[str(idx)](group_data)\n",
    "            for i, feature_index in enumerate(group):\n",
    "                predictions[feature_index].append(pred[:, :, i:i+1])\n",
    "\n",
    "        # Averaging predictions for each feature\n",
    "        for i in range(x.size(2)):\n",
    "            if predictions[i]:\n",
    "                # Stack predictions and take the mean across the predictions for each feature\n",
    "                output[:, :, i:i+1] = torch.mean(torch.stack(predictions[i], dim=0), dim=0)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for seq_x, seq_y, seq_x_mark, seq_y_mark in test_loader:\n",
    "            seq_x, seq_y = seq_x.to(device), seq_y.to(device)\n",
    "            outputs = model(seq_x)\n",
    "            loss = criterion(outputs, seq_y)\n",
    "            total_loss += loss.item() * seq_x.size(0)\n",
    "    return total_loss / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for seq_x, seq_y, seq_x_mark, seq_y_mark in train_loader:\n",
    "        seq_x, seq_y = seq_x.to(device), seq_y.to(device)\n",
    "        print(seq_x.shape)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(seq_x)\n",
    "        loss = criterion(outputs, seq_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * seq_x.size(0)\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for seq_x, seq_y, seq_x_mark, seq_y_mark in val_loader:\n",
    "            seq_x, seq_y = seq_x.to(device), seq_y.to(device)\n",
    "            outputs = model(seq_x)\n",
    "            loss = criterion(outputs, seq_y)\n",
    "            total_loss += loss.item() * seq_x.size(0)\n",
    "    return total_loss / len(val_loader.dataset)\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                           Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "    \n",
    "# seq_len, pred_len, n_points, dropout, wavelet_j, wavelet, subgraph_size, node_dim, n_gnn_layer\n",
    "# Assuming DWT_MLP_Model is defined elsewhere, along with the necessary imports\n",
    "seq_ = 24*4\n",
    "pred_ = 24*4\n",
    "n_points = 321\n",
    "wavelet_j = 2\n",
    "wavelet = 'haar'\n",
    "dropout_rate = 0.05\n",
    "supbraph_size = 6\n",
    "node_dim = 40\n",
    "n_gnn_layer = 3\n",
    "s_size = 5\n",
    "indices = [0,1,2,3,4,5] #np.random.choice(range(100), size=3, replace=False)\n",
    "input_length = [24*4,512]\n",
    "\n",
    "\n",
    "\n",
    "for i in indices:\n",
    "    if i < 3:\n",
    "        seq_length = input_length[0]\n",
    "    else:\n",
    "        seq_length = input_length[1]\n",
    "        \n",
    "    # Specify the file path\n",
    "    root_path = '/home/choi/Wave_Transformer/optuna_/electricity/'\n",
    "    data_path = 'electricity.csv'\n",
    "    # Size parameters\n",
    "\n",
    "    total_x = load_dataset(root_path, data_path, drop_columns=['date'])\n",
    "    \n",
    "    # Compute autocorrelation matrix\n",
    "    autocorrelation_matrix = compute_autocorrelation(total_x)\n",
    "    correlated_features = extract_correlated_features(autocorrelation_matrix)\n",
    "    seq_len = seq_length # 24*4*4\n",
    "    pred_len = 24*4\n",
    "    #batch_size = bs\n",
    "    # Initialize the custom dataset for training, validation, and testing\n",
    "    train_dataset = Dataset_Custom(root_path=root_path, features= 'M', flag='train', data_path=data_path, step_size =s_size, size = [seq_len, pred_len])\n",
    "    val_dataset = Dataset_Custom(root_path=root_path, features= 'M',flag='val', data_path=data_path,step_size = s_size)\n",
    "    test_dataset = Dataset_Custom(root_path=root_path, features= 'M',flag='test', data_path=data_path,step_size = s_size)\n",
    "\n",
    "    # Optionally, initialize the dataset for prediction (if needed)\n",
    "    #pred_dataset = Dataset_Pred(root_path=root_path, flag='pred', size=size, data_path=data_path, inverse=True)\n",
    "\n",
    "    # Example on how to create DataLoaders for PyTorch training (adjust batch_size as needed)\n",
    "    batch_size = 8\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last = True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last = False)\n",
    "    dropout_rate = dropout_rate\n",
    "    \n",
    "    model = Model(\n",
    "    seq_len=seq_len, \n",
    "    pred_len=pred_len, \n",
    "    n_points=n_points, \n",
    "    dropout=dropout_rate, \n",
    "    wavelet_j=wavelet_j, \n",
    "    wavelet=wavelet, \n",
    "    subgraph_size=supbraph_size, \n",
    "    node_dim=node_dim, \n",
    "    n_gnn_layer=n_gnn_layer,\n",
    "    correlated_groups=correlated_features\n",
    ")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Assuming you have a device (GPU/CPU) setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "    num_epochs = 50  # or any other number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = validate(model, val_loader, criterion, device)\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Call early stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "    test_loss = test(model, test_loader, criterion, device)\n",
    "    print(f'Test Loss: {test_loss:.4f}')\n",
    "\n",
    "\n",
    "        \n",
    "      \n",
    "    \n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([  0,  46, 119, 123, 125, 117]),\n",
       " 1: array([  1, 197, 282, 268, 263, 276]),\n",
       " 2: array([  2,  69,  46, 146, 114, 147]),\n",
       " 3: array([ 3, 66, 23, 65, 35, 67]),\n",
       " 4: array([ 4, 23, 53, 63, 67, 66]),\n",
       " 5: array([ 5, 62, 74, 40, 63, 61]),\n",
       " 6: array([  6,  70,  64,  30,  77, 115]),\n",
       " 7: array([ 7, 14, 45, 40, 73, 18]),\n",
       " 8: array([ 8, 38, 15, 72, 45, 10]),\n",
       " 9: array([ 9, 38, 75, 15, 68, 41]),\n",
       " 10: array([10, 74, 61, 63,  5, 62]),\n",
       " 11: array([ 11, 124,  29, 126,  75, 123]),\n",
       " 12: array([12, 68, 75, 86, 85, 87]),\n",
       " 13: array([13, 49, 65, 47, 74, 67]),\n",
       " 14: array([14, 45, 40, 72,  7, 62]),\n",
       " 15: array([15, 72, 45, 78, 38, 14]),\n",
       " 16: array([16, 74, 47, 65, 49, 35]),\n",
       " 17: array([17, 67, 74,  3,  5, 63]),\n",
       " 18: array([18, 40, 14, 45, 62,  5]),\n",
       " 19: array([19, 75, 38, 68, 31, 11]),\n",
       " 20: array([20, 44, 47, 71,  3, 65]),\n",
       " 21: array([21, 72, 26,  5, 45, 47]),\n",
       " 22: array([22, 74, 61, 35, 66,  3]),\n",
       " 23: array([23,  3, 67, 66, 61, 53]),\n",
       " 24: array([24, 65, 53, 66, 74, 67]),\n",
       " 25: array([ 25,  36, 103,  68,  75, 319]),\n",
       " 26: array([26,  5, 40, 62, 61, 74]),\n",
       " 27: array([27, 56, 54, 66, 23, 49]),\n",
       " 28: array([28, 65, 66,  3, 47, 74]),\n",
       " 29: array([ 29,  11, 124,  99,  87,  68]),\n",
       " 30: array([30, 77, 49, 47, 13, 58]),\n",
       " 31: array([31, 41, 38, 55,  7, 75]),\n",
       " 32: array([ 32,  21,  43,  26, 104,  76]),\n",
       " 33: array([33, 45, 40, 62, 14, 72]),\n",
       " 34: array([34, 62, 40, 14, 45,  5]),\n",
       " 35: array([35, 74, 65, 66,  3, 62]),\n",
       " 36: array([ 36,  25,  87, 103, 124,  11]),\n",
       " 37: array([37, 74, 53, 62, 63, 61]),\n",
       " 38: array([38, 15, 45, 14, 72, 31]),\n",
       " 39: array([39, 62, 40, 80, 74, 72]),\n",
       " 40: array([40, 62,  5, 74, 45, 72]),\n",
       " 41: array([41, 45, 14, 40, 62,  7]),\n",
       " 42: array([42, 58, 48, 45, 15, 73]),\n",
       " 43: array([43, 74, 47, 61, 10, 63]),\n",
       " 44: array([44, 47, 65, 49, 71, 76]),\n",
       " 45: array([45, 40, 14, 72,  5, 62]),\n",
       " 46: array([ 46,  69,  59, 182,  16,  40]),\n",
       " 47: array([47, 44, 65, 74, 35, 61]),\n",
       " 48: array([48, 73, 45, 15, 14,  7]),\n",
       " 49: array([49, 13, 47, 44, 65, 71]),\n",
       " 50: array([ 50,  52,   4, 134,  63,  51]),\n",
       " 51: array([51, 74, 67, 10,  5, 63]),\n",
       " 52: array([52, 63,  4, 50, 51, 10]),\n",
       " 53: array([53, 74, 67, 24, 62, 63]),\n",
       " 54: array([54, 22, 65, 66, 35, 56]),\n",
       " 55: array([55, 31, 15, 48, 45, 38]),\n",
       " 56: array([56, 66,  3, 65, 67, 13]),\n",
       " 57: array([ 57, 122,  32,  46,  69,  30]),\n",
       " 58: array([58, 45, 47, 44, 72, 77]),\n",
       " 59: array([ 59,  46, 310, 182,  69, 146]),\n",
       " 60: array([60, 63, 67, 10, 61, 74]),\n",
       " 61: array([61, 74, 63,  5, 65, 62]),\n",
       " 62: array([62, 40,  5, 74, 63, 45]),\n",
       " 63: array([63, 74,  5, 61, 62, 67]),\n",
       " 64: array([64, 77, 58, 44, 70, 78]),\n",
       " 65: array([65, 66, 24, 61, 35,  3]),\n",
       " 66: array([66, 65,  3, 35, 24, 67]),\n",
       " 67: array([67, 74, 63, 53, 61,  3]),\n",
       " 68: array([68, 75, 12, 86, 11, 99]),\n",
       " 69: array([69, 61, 10, 40, 26, 74]),\n",
       " 70: array([70, 64,  6, 77, 42, 48]),\n",
       " 71: array([71, 44, 47, 49, 13, 65]),\n",
       " 72: array([72, 40, 45,  5, 14, 62]),\n",
       " 73: array([73,  7, 48, 14, 45, 15]),\n",
       " 74: array([74, 62,  5, 61, 40, 63]),\n",
       " 75: array([ 75,  68,  12,  11, 124,  86]),\n",
       " 76: array([76, 65, 44, 61, 66, 13]),\n",
       " 77: array([77, 47, 44, 49, 58, 48]),\n",
       " 78: array([78, 15, 72, 45, 58, 38]),\n",
       " 79: array([79, 62, 45, 40,  5, 74]),\n",
       " 80: array([80, 74, 62,  5, 40, 53]),\n",
       " 81: array([ 81,  25, 103,  36, 319, 132]),\n",
       " 82: array([ 82, 301, 201, 260,  90, 185]),\n",
       " 83: array([ 83,  11, 123, 124,  29,  99]),\n",
       " 84: array([ 84,  86, 103, 126, 124,  85]),\n",
       " 85: array([ 85,  12,  86, 126,  31,  68]),\n",
       " 86: array([86, 12, 68, 85, 11, 75]),\n",
       " 87: array([87, 12, 11, 86, 29, 68]),\n",
       " 88: array([ 88, 292,  93, 314, 291, 306]),\n",
       " 89: array([ 89,  91,  93, 306, 290, 291]),\n",
       " 90: array([ 90,  95, 158,  94, 181, 159]),\n",
       " 91: array([ 91,  89, 306, 290,  94,  93]),\n",
       " 92: array([ 92, 304, 100, 294,  88, 311]),\n",
       " 93: array([ 93, 292, 306, 308,  94,  89]),\n",
       " 94: array([ 94,  93, 302, 292, 314,  91]),\n",
       " 95: array([ 95,  94, 181, 294, 314, 312]),\n",
       " 96: array([96, 40, 22,  5, 62, 74]),\n",
       " 97: array([ 97, 270, 111, 278, 147, 158]),\n",
       " 98: array([ 98, 313,  89, 300, 294, 290]),\n",
       " 99: array([ 99,  86,  11,  29, 126,  68]),\n",
       " 100: array([100, 101, 304, 306, 311,  89]),\n",
       " 101: array([101, 306, 100,  93, 297, 102]),\n",
       " 102: array([102, 187, 186,  93, 306, 162]),\n",
       " 103: array([103, 126, 124,  11,  29, 123]),\n",
       " 104: array([104, 116,  32, 118, 115,  85]),\n",
       " 105: array([105, 107, 103, 126, 106,  25]),\n",
       " 106: array([106, 105, 107, 103,  11, 124]),\n",
       " 107: array([107, 103, 126, 124,  11,  36]),\n",
       " 108: array([108, 109, 110, 211, 137, 249]),\n",
       " 109: array([109, 108, 110, 211, 137, 142]),\n",
       " 110: array([110, 108, 109, 211, 233, 179]),\n",
       " 111: array([111,  97, 270, 278, 116, 184]),\n",
       " 112: array([112, 228, 110, 269, 279, 268]),\n",
       " 113: array([113,  12,  87,  86,  75,  68]),\n",
       " 114: array([114, 270,  97, 116, 147, 115]),\n",
       " 115: array([115, 116, 278, 111, 125,  97]),\n",
       " 116: array([116, 111, 115,  97, 270, 104]),\n",
       " 117: array([117, 119, 122, 120, 320, 152]),\n",
       " 118: array([118, 125, 116, 104, 128, 115]),\n",
       " 119: array([119, 117, 120, 320, 278, 152]),\n",
       " 120: array([120, 119, 117, 126, 103, 320]),\n",
       " 121: array([121, 146, 122, 182, 119, 117]),\n",
       " 122: array([122, 117, 182, 119, 120, 146]),\n",
       " 123: array([123, 126,  11, 124, 108, 103]),\n",
       " 124: array([124,  11, 126, 123, 103,  75]),\n",
       " 125: array([125, 119, 115, 278, 117, 320]),\n",
       " 126: array([126, 124, 103, 123,  11,  86]),\n",
       " 127: array([127, 128,  11, 147, 123, 270]),\n",
       " 128: array([128, 124, 133, 127,  11, 111]),\n",
       " 129: array([129, 132, 319, 130,  25,  96]),\n",
       " 130: array([130, 132, 129, 319, 134,  25]),\n",
       " 131: array([131, 134, 147, 146, 122,  57]),\n",
       " 132: array([132, 129, 319, 130, 134,  25]),\n",
       " 133: array([133, 124,  11, 128, 126, 103]),\n",
       " 134: array([134, 132,  52, 130,  50, 129]),\n",
       " 135: array([135, 186, 187, 171, 150, 281]),\n",
       " 136: array([136, 259, 141, 206, 208, 209]),\n",
       " 137: array([137, 142, 141, 249, 243, 215]),\n",
       " 138: array([138, 141, 206, 201, 233, 191]),\n",
       " 139: array([139, 141, 220, 137, 249, 195]),\n",
       " 140: array([140, 212, 243, 199, 137, 233]),\n",
       " 141: array([141, 233, 142, 201, 220, 206]),\n",
       " 142: array([142, 201, 137, 141, 233, 243]),\n",
       " 143: array([143, 220, 233, 209, 144, 267]),\n",
       " 144: array([144, 201, 267, 206, 233, 191]),\n",
       " 145: array([145, 210, 213, 215, 262, 206]),\n",
       " 146: array([146, 182, 147, 178, 270, 122]),\n",
       " 147: array([147, 270, 140, 178, 137, 142]),\n",
       " 148: array([148, 167, 159, 151, 154, 168]),\n",
       " 149: array([149, 159, 177, 152, 174, 181]),\n",
       " 150: array([150, 187, 186, 171, 181, 175]),\n",
       " 151: array([151, 168, 171, 159, 173, 243]),\n",
       " 152: array([152, 149, 175, 159, 181, 167]),\n",
       " 153: array([153, 163, 172, 183, 175, 167]),\n",
       " 154: array([154, 237, 171, 280, 254, 242]),\n",
       " 155: array([155, 173, 185, 191, 183, 174]),\n",
       " 156: array([156, 243, 262, 271, 249, 233]),\n",
       " 157: array([157, 177, 166, 173, 170, 165]),\n",
       " 158: array([158, 175, 181, 167, 163, 159]),\n",
       " 159: array([159, 181, 171, 161, 167, 186]),\n",
       " 160: array([160, 243, 151, 173, 165, 172]),\n",
       " 161: array([161, 159, 181, 170, 166, 171]),\n",
       " 162: array([162, 187, 181, 281,  93, 167]),\n",
       " 163: array([163, 181, 175, 171, 167, 186]),\n",
       " 164: array([164, 249, 171, 212, 243, 262]),\n",
       " 165: array([165, 179, 243, 257, 262, 166]),\n",
       " 166: array([166, 173, 161, 170, 171, 168]),\n",
       " 167: array([167, 181, 171, 163, 175, 170]),\n",
       " 168: array([168, 187, 186, 171, 159, 151]),\n",
       " 169: array([169, 254, 262, 257, 171, 228]),\n",
       " 170: array([170, 171, 167, 161, 181, 163]),\n",
       " 171: array([171, 170, 181, 187, 159, 167]),\n",
       " 172: array([172, 163, 171, 170, 173, 161]),\n",
       " 173: array([173, 166, 155, 180, 283, 172]),\n",
       " 174: array([174, 185, 280, 159, 171, 183]),\n",
       " 175: array([175, 186, 181, 163, 176, 150]),\n",
       " 176: array([176, 175, 181, 180, 177, 173]),\n",
       " 177: array([177, 181, 171, 159, 170, 167]),\n",
       " 178: array([178, 283, 270, 160, 166, 276]),\n",
       " 179: array([179, 272, 243, 262, 273, 283]),\n",
       " 180: array([180, 283, 179, 272, 176, 191]),\n",
       " 181: array([181, 159, 175, 171, 163, 177]),\n",
       " 182: array([182, 146, 122,  46, 310,  59]),\n",
       " 183: array([183, 175, 163, 280, 181, 159]),\n",
       " 184: array([184, 181, 167, 175, 163, 150]),\n",
       " 185: array([185, 191, 279, 280, 271, 174]),\n",
       " 186: array([186, 187, 175, 150, 168, 171]),\n",
       " 187: array([187, 186, 171, 150, 168, 281]),\n",
       " 188: array([188, 283, 202, 272, 268, 276]),\n",
       " 189: array([189, 205, 276, 233, 283, 202]),\n",
       " 190: array([190, 262, 266, 206, 191, 233]),\n",
       " 191: array([191, 233, 185, 179, 262, 272]),\n",
       " 192: array([192, 283, 272, 179, 273, 271]),\n",
       " 193: array([193, 258, 256, 189, 250, 136]),\n",
       " 194: array([194, 233, 276, 179, 243, 199]),\n",
       " 195: array([195, 233, 206, 262, 215, 220]),\n",
       " 196: array([196, 228, 261, 280, 242, 204]),\n",
       " 197: array([197, 283, 272, 273, 208, 202]),\n",
       " 198: array([198, 283, 210, 204, 262, 234]),\n",
       " 199: array([199, 233, 179, 283, 272, 268]),\n",
       " 200: array([200, 233, 191, 259, 141, 201]),\n",
       " 201: array([201, 233, 191, 206, 262, 179]),\n",
       " 202: array([202, 283, 233, 272, 199, 191]),\n",
       " 203: array([203, 242, 228, 273, 257, 267]),\n",
       " 204: array([204, 234, 233, 279, 271, 262]),\n",
       " 205: array([205, 233, 272, 189, 283, 276]),\n",
       " 206: array([206, 262, 233, 235, 215, 191]),\n",
       " 207: array([207, 139, 178, 137, 198, 189]),\n",
       " 208: array([208, 262, 213, 215, 257, 210]),\n",
       " 209: array([209, 237, 242, 239, 228, 282]),\n",
       " 210: array([210, 262, 213, 215, 208, 261]),\n",
       " 211: array([211, 243, 249, 220, 262, 266]),\n",
       " 212: array([212, 215, 221, 249, 243, 262]),\n",
       " 213: array([213, 262, 215, 210, 208, 206]),\n",
       " 214: array([214, 212, 235, 206, 233, 191]),\n",
       " 215: array([215, 262, 212, 221, 213, 210]),\n",
       " 216: array([216, 181, 230, 163, 185, 257]),\n",
       " 217: array([217, 191, 233, 225, 185, 262]),\n",
       " 218: array([218, 215, 220, 206, 262, 233]),\n",
       " 219: array([219, 228, 280, 241, 238, 279]),\n",
       " 220: array([220, 262, 215, 266, 206, 243]),\n",
       " 221: array([221, 212, 215, 249, 262, 243]),\n",
       " 222: array([222, 233, 262, 206, 261, 221]),\n",
       " 223: array([223, 222, 261, 235, 233, 206]),\n",
       " 224: array([224, 210, 220, 261, 250, 213]),\n",
       " 225: array([225, 191, 217, 185, 233, 208]),\n",
       " 226: array([226, 228, 271, 237, 242, 280]),\n",
       " 227: array([227, 262, 206, 208, 213, 215]),\n",
       " 228: array([228, 242, 280, 239, 241, 257]),\n",
       " 229: array([229, 242, 281, 187, 237, 234]),\n",
       " 230: array([230, 281, 181, 237, 171, 159]),\n",
       " 231: array([231, 244, 237, 242, 240, 282]),\n",
       " 232: array([232, 242, 234, 273, 271, 257]),\n",
       " 233: array([233, 191, 279, 272, 206, 262]),\n",
       " 234: array([234, 242, 237, 262, 273, 254]),\n",
       " 235: array([235, 206, 262, 233, 267, 214]),\n",
       " 236: array([236, 262, 257, 242, 215, 208]),\n",
       " 237: array([237, 244, 242, 254, 271, 280]),\n",
       " 238: array([238, 228, 280, 241, 239, 237]),\n",
       " 239: array([239, 228, 208, 242, 237, 244]),\n",
       " 240: array([240, 244, 237, 254, 242, 277]),\n",
       " 241: array([241, 280, 228, 269, 279, 273]),\n",
       " 242: array([242, 237, 228, 257, 171, 262]),\n",
       " 243: array([243, 262, 249, 179, 212, 233]),\n",
       " 244: array([244, 237, 240, 242, 254, 280]),\n",
       " 245: array([245, 140, 147, 199, 184, 137]),\n",
       " 246: array([246, 260, 250, 258, 137, 220]),\n",
       " 247: array([247, 154, 224, 220, 266, 253]),\n",
       " 248: array([248, 261, 228, 264, 209, 226]),\n",
       " 249: array([249, 243, 212, 262, 215, 221]),\n",
       " 250: array([250, 220, 191, 233, 206, 235]),\n",
       " 251: array([251, 259, 222, 206, 220, 250]),\n",
       " 252: array([252, 262, 191, 190, 272, 179]),\n",
       " 253: array([253, 262, 261, 215, 220, 243]),\n",
       " 254: array([254, 237, 257, 242, 244, 262]),\n",
       " 255: array([255, 279, 228, 250, 233, 191]),\n",
       " 256: array([256, 259, 233, 253, 261, 220]),\n",
       " 257: array([257, 262, 242, 208, 254, 243]),\n",
       " 258: array([258, 256, 251, 250, 260, 222]),\n",
       " 259: array([259, 262, 233, 267, 220, 206]),\n",
       " 260: array([260, 141, 208, 233, 250, 220]),\n",
       " 261: array([261, 262, 210, 257, 208, 213]),\n",
       " 262: array([262, 215, 206, 243, 208, 190]),\n",
       " 263: array([263, 257, 267, 233, 273, 228]),\n",
       " 264: array([264, 234, 280, 262, 228, 254]),\n",
       " 265: array([265, 208, 262, 257, 261, 215]),\n",
       " 266: array([266, 190, 262, 220, 179, 206]),\n",
       " 267: array([267, 271, 273, 279, 233, 272]),\n",
       " 268: array([268, 273, 272, 269, 279, 267]),\n",
       " 269: array([269, 273, 268, 272, 279, 242]),\n",
       " 270: array([270, 147,  97, 178, 140, 184]),\n",
       " 271: array([271, 267, 237, 279, 233, 273]),\n",
       " 272: array([272, 283, 273, 233, 179, 191]),\n",
       " 273: array([273, 272, 267, 279, 233, 268]),\n",
       " 274: array([274, 228, 257, 242, 271, 171]),\n",
       " 275: array([275, 273, 233, 179, 272, 267]),\n",
       " 276: array([276, 233, 272, 283, 268, 273]),\n",
       " 277: array([277, 237, 281, 244, 280, 240]),\n",
       " 278: array([278, 268, 176, 183, 158, 216]),\n",
       " 279: array([279, 233, 267, 271, 273, 185]),\n",
       " 280: array([280, 237, 185, 228, 241, 267]),\n",
       " 281: array([281, 181, 230, 187, 186, 171]),\n",
       " 282: array([282, 242, 227, 237, 239, 257]),\n",
       " 283: array([283, 272, 179, 271, 273, 233]),\n",
       " 284: array([284, 280, 185, 191, 233, 241]),\n",
       " 285: array([285, 286, 229, 209, 237, 162]),\n",
       " 286: array([286,  93, 285, 292, 288, 308]),\n",
       " 287: array([287, 173, 294,  95,  90, 283]),\n",
       " 288: array([288, 286,  93, 154, 292, 157]),\n",
       " 289: array([289, 242, 281, 187, 162, 171]),\n",
       " 290: array([290,  89,  91, 309, 306, 313]),\n",
       " 291: array([291,  93, 292, 308,  89, 306]),\n",
       " 292: array([292,  93, 308, 291, 306, 296]),\n",
       " 293: array([293, 306, 308,  93, 311,  91]),\n",
       " 294: array([294,  89,  95, 313,  91, 302]),\n",
       " 295: array([295,  89, 302,  93, 291,  88]),\n",
       " 296: array([296, 297, 292,  93, 306, 308]),\n",
       " 297: array([297, 296,  93, 292, 306, 291]),\n",
       " 298: array([298, 182, 146, 117,  32, 310]),\n",
       " 299: array([299, 124, 103, 107,  36, 126]),\n",
       " 300: array([300,  89, 290, 294,  91, 313]),\n",
       " 301: array([301,  94, 312, 314, 292,  88]),\n",
       " 302: array([302,  94,  93,  91,  89, 292]),\n",
       " 303: array([303, 309,  91, 313, 290, 311]),\n",
       " 304: array([304, 306, 311, 292,  88,  93]),\n",
       " 305: array([305, 306,  89,  93, 181, 292]),\n",
       " 306: array([306,  93,  89, 292,  91, 291]),\n",
       " 307: array([307, 294, 313,  91,  90, 181]),\n",
       " 308: array([308,  93, 292, 291,  94, 306]),\n",
       " 309: array([309, 290,  91,  89, 306, 313]),\n",
       " 310: array([310, 225, 217, 191, 119,  59]),\n",
       " 311: array([311, 306,  89,  91, 291,  93]),\n",
       " 312: array([312,  94, 181, 292,  95, 308]),\n",
       " 313: array([313, 294, 290, 175, 309, 300]),\n",
       " 314: array([314,  94,  88,  91,  89,  93]),\n",
       " 315: array([315, 181, 175,  95,  94, 312]),\n",
       " 316: array([316,  94, 314, 181,  91,  93]),\n",
       " 317: array([317, 319, 132, 129, 130, 320]),\n",
       " 318: array([318, 320, 117, 128, 125, 132]),\n",
       " 319: array([319, 129, 132, 130,  25, 103]),\n",
       " 320: array([320, 119, 313, 117, 152, 278])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
